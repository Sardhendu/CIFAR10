{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import Tools\n",
    "# from Tools import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def NNetmodel(\n",
    "    num_output_unit,\n",
    "    num_hid1_unit=500, \n",
    "    num_hid2_unit=100,\n",
    "    momentum=0.9,\n",
    "    learning_rate=0.1\n",
    "    ):\n",
    "    seed = 128\n",
    "    rng = np.random.RandomState(seed)\n",
    "    reset_graph()\n",
    "    \n",
    "    print (num_output_unit)\n",
    "    print (num_hid1_unit)\n",
    "    print (num_hid2_unit)\n",
    "    print (momentum)\n",
    "    print (learning_rate)\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None,None], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.float32, shape=[None,None], name='output_placeholder')\n",
    "    num_input_unit = tf.shape(x)[1]   # x is of shape batch_size * no_of_features\n",
    "    \n",
    "    weights = {\n",
    "        'input_to_hid1_wghts' : tf.Variable(tf.random_normal([1024,num_hid1_unit], seed=seed)),\n",
    "        'hid1_to_output_wghts' : tf.Variable(tf.random_normal([num_hid1_unit,num_output_unit], seed=seed))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'hid1_bias': tf.Variable(tf.zeros([num_hid1_unit])),\n",
    "        'output_bias' : tf.Variable(tf.zeros([num_output_unit]))\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Forward Propagate:\n",
    "    input_to_hid1 = tf.matmul(x, weights['input_to_hid1_wghts']) + biases['hid1_bias']\n",
    "    hid1_state = tf.sigmoid(input_to_hid1)\n",
    "    \n",
    "    hid1_to_output = tf.matmul(hid1_state, weights['hid1_to_output_wghts']) + biases['output_bias']\n",
    "    output_state = tf.nn.softmax(hid1_to_output)\n",
    "    \n",
    "    # Error Derivative\n",
    "    err_deriv = tf.sub(output_state, y, name=None)\n",
    "    \n",
    "    loss_CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hid1_to_output, y))\n",
    "\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_CE)\n",
    "    \n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, \n",
    "                                            momentum, \n",
    "                                            use_locking=False, \n",
    "                                            name='Momentum', \n",
    "                                            use_nesterov=True).minimize(loss_CE)\n",
    "    \n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        loss = loss_CE,\n",
    "        optimizer = optimizer,\n",
    "        prediction = output_state,\n",
    "        num_input_unit = num_input_unit\n",
    "#         saver = tf.train.Saver()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the Network\n",
    "def trainNetwork(graph_dict):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        training_loss = 0\n",
    "        epochs = 10\n",
    "        for epoch in np.arange(epochs):\n",
    "            for trnData, trnLabels in batch_file_iterator(path_to_batches, batch_filename_arr):\n",
    "                num_features = trnData.shape[1]*trnData.shape[2]\n",
    "                trnDataRshp, trnLabelsRshp = Tools.reshape_data(trnData, \n",
    "                                                          trnLabels, \n",
    "                                                          num_features=num_features,\n",
    "                                                          num_labels = 10\n",
    "                                                         )\n",
    "                                    \n",
    "#                 print (trnData)\n",
    "#                 print (trnLabels)#trnData.shape[2])\n",
    "                feed_dict = {\n",
    "                    graph_dict['x']:trnDataRshp, \n",
    "                    graph_dict['y']:trnLabelsRshp\n",
    "                }\n",
    "\n",
    "                loss_, opt_, pred_, aa= sess.run([graph_dict['loss'],\n",
    "                                              graph_dict['optimizer'],\n",
    "                                              graph_dict['prediction'],\n",
    "                                              graph_dict['num_input_unit']], \n",
    "                                              feed_dict=feed_dict)\n",
    "                print (aa)\n",
    "                training_loss += loss_\n",
    "                acc = Tools.accuracy(prediction=pred_, labels=trnLabelsRshp, labels_one_hot='Yes')\n",
    "                print (training_loss)\n",
    "                print (acc)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch9.pickle' 'batch4.pickle' 'batch0.pickle' 'batch6.pickle'\n",
      " 'batch7.pickle' 'batch5.pickle' 'batch1.pickle' 'batch3.pickle'\n",
      " 'batch8.pickle' 'batch2.pickle']\n",
      "10\n",
      "100\n",
      "100\n",
      "0.9\n",
      "0.1\n",
      "1024\n",
      "10.466843605\n",
      "7.25\n",
      "1024\n",
      "19.9737930298\n",
      "7.3\n",
      "1024\n",
      "28.5198564529\n",
      "7.625\n",
      "1024\n",
      "36.1298980713\n",
      "8.65\n",
      "1024\n",
      "43.2476010323\n",
      "7.925\n",
      "1024\n",
      "49.7398619652\n",
      "8.975\n",
      "1024\n",
      "55.8409657478\n",
      "10.25\n",
      "1024\n",
      "61.6292657852\n",
      "10.725\n",
      "1024\n",
      "67.2206835747\n",
      "10.975\n",
      "1024\n",
      "72.6140298843\n",
      "12.075\n",
      "1024\n",
      "77.9640345573\n",
      "12.175\n",
      "1024\n",
      "83.1613073349\n",
      "14.125\n",
      "1024\n",
      "88.2264742851\n",
      "13.8\n",
      "1024\n",
      "93.2313694954\n",
      "13.925\n",
      "1024\n",
      "98.2056474686\n",
      "14.6\n",
      "1024\n",
      "103.043973446\n",
      "13.75\n",
      "1024\n",
      "107.725228786\n",
      "15.35\n",
      "1024\n",
      "112.399525642\n",
      "15.5\n",
      "1024\n",
      "117.066029549\n",
      "15.375\n",
      "1024\n",
      "121.570713043\n",
      "15.675\n",
      "1024\n",
      "126.013179779\n",
      "15.825\n",
      "1024\n",
      "130.352700233\n",
      "16.775\n",
      "1024\n",
      "134.690052986\n",
      "16.2\n",
      "1024\n",
      "139.024157524\n",
      "16.375\n",
      "1024\n",
      "143.375281334\n",
      "16.775\n",
      "1024\n",
      "147.662895203\n",
      "15.275\n",
      "1024\n",
      "151.813127995\n",
      "17.325\n",
      "1024\n",
      "155.96649313\n",
      "17.025\n",
      "1024\n",
      "160.078063965\n",
      "17.225\n",
      "1024\n",
      "164.06761837\n",
      "16.55\n",
      "1024\n",
      "168.03015995\n",
      "17.475\n",
      "1024\n",
      "171.931862354\n",
      "18.275\n",
      "1024\n",
      "175.818987846\n",
      "17.0\n",
      "1024\n",
      "179.7420187\n",
      "17.4\n",
      "1024\n",
      "183.679896355\n",
      "17.25\n",
      "1024\n",
      "187.589126825\n",
      "15.7\n",
      "1024\n",
      "191.389971972\n",
      "17.375\n",
      "1024\n",
      "195.202420235\n",
      "16.95\n",
      "1024\n",
      "198.979377508\n",
      "17.375\n",
      "1024\n",
      "202.65100193\n",
      "17.05\n",
      "1024\n",
      "206.328312635\n",
      "17.05\n",
      "1024\n",
      "209.944048166\n",
      "18.05\n",
      "1024\n",
      "213.543261766\n",
      "17.85\n",
      "1024\n",
      "217.187180519\n",
      "17.4\n",
      "1024\n",
      "220.835697651\n",
      "18.1\n",
      "1024\n",
      "224.479337215\n",
      "16.35\n",
      "1024\n",
      "228.01461792\n",
      "17.575\n",
      "1024\n",
      "231.570655107\n",
      "17.425\n",
      "1024\n",
      "235.101988316\n",
      "18.25\n",
      "1024\n",
      "238.54172802\n",
      "18.225\n",
      "1024\n",
      "241.994570255\n",
      "18.025\n",
      "1024\n",
      "245.392014265\n",
      "19.1\n",
      "1024\n",
      "248.779970407\n",
      "18.35\n",
      "1024\n",
      "252.21581912\n",
      "18.15\n",
      "1024\n",
      "255.649670601\n",
      "18.775\n",
      "1024\n",
      "259.093343019\n",
      "16.625\n",
      "1024\n",
      "262.436214685\n",
      "18.525\n",
      "1024\n",
      "265.801601887\n",
      "18.025\n",
      "1024\n",
      "269.151635647\n",
      "18.7\n",
      "1024\n",
      "272.421554089\n",
      "18.525\n",
      "1024\n",
      "275.704496622\n",
      "18.425\n",
      "1024\n",
      "278.94121623\n",
      "19.35\n",
      "1024\n",
      "282.167749882\n",
      "18.4\n",
      "1024\n",
      "285.437823057\n",
      "18.625\n",
      "1024\n",
      "288.70393157\n",
      "19.45\n",
      "1024\n",
      "291.988137722\n",
      "17.35\n",
      "1024\n",
      "295.176156282\n",
      "18.825\n",
      "1024\n",
      "298.389691353\n",
      "18.55\n",
      "1024\n",
      "301.592296839\n",
      "19.225\n",
      "1024\n",
      "304.723514795\n",
      "18.775\n",
      "1024\n",
      "307.869924068\n",
      "18.875\n",
      "1024\n",
      "310.975117445\n",
      "19.65\n",
      "1024\n",
      "314.071126938\n",
      "18.4\n",
      "1024\n",
      "317.204975128\n",
      "19.225\n",
      "1024\n",
      "320.334865808\n",
      "20.0\n",
      "1024\n",
      "323.488686562\n",
      "17.8\n",
      "1024\n",
      "326.547234774\n",
      "19.45\n",
      "1024\n",
      "329.634109259\n",
      "19.175\n",
      "1024\n",
      "332.713382959\n",
      "19.325\n",
      "1024\n",
      "335.727832079\n",
      "19.575\n",
      "1024\n",
      "338.758026838\n",
      "19.35\n",
      "1024\n",
      "341.750085831\n",
      "20.3\n",
      "1024\n",
      "344.73531723\n",
      "19.275\n",
      "1024\n",
      "347.754452229\n",
      "19.425\n",
      "1024\n",
      "350.768917322\n",
      "20.45\n",
      "1024\n",
      "353.810956478\n",
      "18.325\n",
      "1024\n",
      "356.760207176\n",
      "19.975\n",
      "1024\n",
      "359.738435745\n",
      "19.9\n",
      "1024\n",
      "362.712257862\n",
      "19.875\n",
      "1024\n",
      "365.626503944\n",
      "19.7\n",
      "1024\n",
      "368.556192636\n",
      "19.975\n",
      "1024\n",
      "371.45106411\n",
      "20.725\n",
      "1024\n",
      "374.340349197\n",
      "19.725\n",
      "1024\n",
      "377.260159731\n",
      "19.725\n",
      "1024\n",
      "380.175125122\n",
      "20.875\n",
      "1024\n",
      "383.120064497\n",
      "18.775\n",
      "1024\n",
      "385.974683046\n",
      "20.25\n",
      "1024\n",
      "388.858734369\n",
      "20.3\n",
      "1024\n",
      "391.740625381\n",
      "20.075\n",
      "1024\n",
      "394.567277193\n",
      "20.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "parent_dir = parent_dir = '/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/'\n",
    "fetaureType = 'featureSTD' \n",
    "path_to_batches = parent_dir + fetaureType + '/batchPath'\n",
    "batch_filename_arr = np.array(os.listdir(path_to_batches))\n",
    "np.random.shuffle(batch_filename_arr)\n",
    "print (batch_filename_arr)\n",
    "\n",
    "graph_dict = NNetmodel(num_output_unit=10,\n",
    "                       num_hid1_unit=100, \n",
    "                       momentum=0.9,\n",
    "                       learning_rate=0.1\n",
    "                      )\n",
    "# print (graph_dict)\n",
    "trainNetwork(graph_dict)#, path_save_model=path_save_model)\n",
    "# sys.stdout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_output_unit = 10\n",
    "no_of_labels = 10\n",
    "no_inp_unit = 1024\n",
    "no_hid_unit = 100\n",
    "learning_rate=0.1\n",
    "momentum = 0.9\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    seed = 128\n",
    "    rng = np.random.RandomState(seed)\n",
    "    tf_training_dataset = tf.placeholder(tf.float32, shape=(None, no_inp_unit))\n",
    "    tf_training_labels = tf.placeholder(tf.float32, shape=(None, no_of_labels))\n",
    "\n",
    "    weights = {\n",
    "        'input_to_hid_wghts': tf.Variable(tf.random_normal([no_inp_unit, no_hid_unit], seed=seed)),\n",
    "        'hid_to_output_wghts': tf.Variable(tf.random_normal([no_hid_unit, no_output_unit], seed=seed))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'hid_bias' : tf.Variable(tf.zeros([no_hid_unit])),\n",
    "        'output_bias' : tf.Variable(tf.zeros([no_output_unit]))\n",
    "    }\n",
    "\n",
    "    ###### Forward Propagate ######\n",
    "    # Hidden Layer\n",
    "    input_to_hid_layer = tf.matmul(tf_training_dataset, weights['input_to_hid_wghts']) + biases['hid_bias']\n",
    "    hid_layer_state = tf.sigmoid(input_to_hid_layer, name=None)\n",
    "\n",
    "    # Output Layer\n",
    "    hid_to_output_layer = tf.matmul(hid_layer_state, weights['hid_to_output_wghts']) + biases['output_bias']\n",
    "    output_layer_state = tf.nn.softmax(hid_to_output_layer, name=None)\n",
    "    error_derivative = tf.sub(output_layer_state, tf_training_labels, name=None)  # -(y - y_hat) = (y_hat - y)\n",
    "    # The above three lines of code can also be combined into one line as below.\n",
    "    loss_CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hid_to_output_layer, tf_training_labels))\n",
    "\n",
    "    \n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, \n",
    "                                            momentum, \n",
    "                                            use_locking=False, \n",
    "                                            name='Momentum', \n",
    "                                            use_nesterov=True).minimize(loss_CE)\n",
    "    # Training Prediction\n",
    "    training_prediction = output_layer_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variable Initialized successfully\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 10.507124\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 9.620941\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 8.508722\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/App-Setup/CondaENV/lib/python3.5/site-packages/ipykernel/__main__.py:19: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at epoch 0: 7.794838\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 6.984330\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 6.502948\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 6.112256\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 5.684135\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 5.508185\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 5.472131\n",
      "Minibatch accuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 1\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"All variable Initialized successfully\")\n",
    "    for epoch in range(epochs):\n",
    "        # Retreive mini-batches\n",
    "        for trnData, trnLabels in batch_file_iterator(path_to_batches, batch_filename_arr):\n",
    "            num_features = trnData.shape[1]*trnData.shape[2]\n",
    "            trnDataRshp, trnLabelsRshp = reshape_data(trnData, \n",
    "                                                          trnLabels, \n",
    "                                                          num_features=num_features,\n",
    "                                                          num_labels = 10\n",
    "                                                         )\n",
    "                                    \n",
    "            print (trnDataRshp.shape)\n",
    "            print (trnLabelsRshp.shape)\n",
    "#                 print (trnData.shape[1], trnData.shape[2])\n",
    "#             feed_dict = {\n",
    "#                     graph_dict['x']:trnDataRshp, \n",
    "#                     graph_dict['y']:trnLabelsRshp\n",
    "#                 }\n",
    "    \n",
    "\n",
    "                \n",
    "            feed_dict = {tf_training_dataset : trnDataRshp, tf_training_labels : trnLabelsRshp}\n",
    "            _, l, predictions = session.run([optimizer, loss_CE, training_prediction], feed_dict=feed_dict)\n",
    "#             print (predictions)\n",
    "        \n",
    "#         if (epoch % 2 == 0) or epoch == epochs-1:\n",
    "            print(\"Minibatch loss at epoch %d: %f\" % (epoch, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, trnLabelsRshp))\n",
    "#             print(\"Validation accuracy: %.1f%%\" % accuracy(crossvalid_prediction.eval(), crossvalid_labels_))\n",
    "#     print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
