{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from DataPreparation import batch_file_iterator\n",
    "# from Tools import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensor flow takes the labels input as binary code, where Alphabet A whose binary value is 0 will turn to a array\n",
    "# with elements [1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0] and B becomes [0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "def reshape_data(dataset, labels, num_features, num_labels, sample_size=None):\n",
    "    if sample_size:\n",
    "        dataset = dataset[:sample_size].reshape(sample_size, num_features) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    else:\n",
    "        dataset = dataset.reshape(len(dataset), num_features) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "def accuracy(prediction, labels, labels_one_hot = None):\n",
    "    # The input labels are a One-Hot Vector\n",
    "    if labels_one_hot:\n",
    "        return (100.0 * np.sum(np.argmax(prediction, 1) == np.argmax(labels, 1)) / prediction.shape[0])\n",
    "    else:\n",
    "        return (100.0 * np.sum(np.argmax(prediction, 1) == np.reshape(labels, [-1])) / prediction.shape[0])\n",
    "\n",
    "    \n",
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def NNetmodel(\n",
    "    num_output_unit,\n",
    "    num_hid1_unit=500, \n",
    "    num_hid2_unit=100,\n",
    "    momentum=0.9,\n",
    "    learning_rate=0.1\n",
    "    ):\n",
    "    seed = 128\n",
    "    rng = np.random.RandomState(seed)\n",
    "    reset_graph()\n",
    "    \n",
    "    print (num_output_unit)\n",
    "    print (num_hid1_unit)\n",
    "    print (num_hid2_unit)\n",
    "    print (momentum)\n",
    "    print (learning_rate)\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None,None], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.float32, shape=[None,None], name='output_placeholder')\n",
    "    num_input_unit = tf.shape(x)[1]   # x is of shape batch_size * no_of_features\n",
    "    \n",
    "    weights = {\n",
    "        'input_to_hid1_wghts' : tf.Variable(tf.random_normal([1024,num_hid1_unit], seed=seed)),\n",
    "        'hid1_to_output_wghts' : tf.Variable(tf.random_normal([num_hid1_unit,num_output_unit], seed=seed))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'hid1_bias': tf.Variable(tf.zeros([num_hid1_unit])),\n",
    "        'output_bias' : tf.Variable(tf.zeros([num_output_unit]))\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Forward Propagate:\n",
    "    input_to_hid1 = tf.matmul(x, weights['input_to_hid1_wghts']) + biases['hid1_bias']\n",
    "    hid1_state = tf.sigmoid(input_to_hid1)\n",
    "    \n",
    "    hid1_to_output = tf.matmul(hid1_state, weights['hid1_to_output_wghts']) + biases['output_bias']\n",
    "    output_state = tf.nn.softmax(hid1_to_output)\n",
    "    \n",
    "    # Error Derivative\n",
    "    err_deriv = tf.sub(output_state, y, name=None)\n",
    "    \n",
    "    loss_CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hid1_to_output, y))\n",
    "\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_CE)\n",
    "    \n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, \n",
    "                                            momentum, \n",
    "                                            use_locking=False, \n",
    "                                            name='Momentum', \n",
    "                                            use_nesterov=True).minimize(loss_CE)\n",
    "    \n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        loss = loss_CE,\n",
    "        optimizer = optimizer,\n",
    "        prediction = output_state,\n",
    "        num_input_unit = num_input_unit\n",
    "#         saver = tf.train.Saver()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the Network\n",
    "def trainNetwork(graph_dict):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        training_loss = 0\n",
    "        epochs = 10\n",
    "        for epoch in np.arange(epochs):\n",
    "            for trnData, trnLabels in batch_file_iterator(path_to_batches, batch_filename_arr):\n",
    "                num_features = trnData.shape[1]*trnData.shape[2]\n",
    "                trnDataRshp, trnLabelsRshp = reshape_data(trnData, \n",
    "                                                          trnLabels, \n",
    "                                                          num_features=num_features,\n",
    "                                                          num_labels = 10\n",
    "                                                         )\n",
    "                                    \n",
    "#                 print (trnData)\n",
    "#                 print (trnLabels)#trnData.shape[2])\n",
    "                feed_dict = {\n",
    "                    graph_dict['x']:trnDataRshp, \n",
    "                    graph_dict['y']:trnLabelsRshp\n",
    "                }\n",
    "\n",
    "                loss_, opt_, pred_, aa= sess.run([graph_dict['loss'],\n",
    "                                              graph_dict['optimizer'],\n",
    "                                              graph_dict['prediction'],\n",
    "                                              graph_dict['num_input_unit']], \n",
    "                                              feed_dict=feed_dict)\n",
    "                print (aa)\n",
    "                training_loss += loss_\n",
    "                acc = accuracy(prediction=pred_, labels=trnLabelsRshp, labels_one_hot='Yes')\n",
    "                print (training_loss)\n",
    "                print (acc)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch1.pickle' 'batch2.pickle' 'batch3.pickle' 'batch7.pickle'\n",
      " 'batch8.pickle' 'batch6.pickle' 'batch9.pickle' 'batch4.pickle'\n",
      " 'batch0.pickle' 'batch5.pickle']\n",
      "10\n",
      "100\n",
      "100\n",
      "0.9\n",
      "0.1\n",
      "1024\n",
      "10.4445772171\n",
      "7.375\n",
      "1024\n",
      "19.9631767273\n",
      "7.55\n",
      "1024\n",
      "28.5068559647\n",
      "7.975\n",
      "1024\n",
      "36.2986965179\n",
      "7.7\n",
      "1024\n",
      "43.3459701538\n",
      "8.925\n",
      "1024\n",
      "49.8398065567\n",
      "9.925\n",
      "1024\n",
      "55.9535698891\n",
      "10.0\n",
      "1024\n",
      "61.6851315498\n",
      "10.175\n",
      "1024\n",
      "67.2041611671\n",
      "11.275\n",
      "1024\n",
      "72.6786751747\n",
      "11.45\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "parent_dir = parent_dir = '/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/'\n",
    "fetaureType = 'featureSTD' \n",
    "path_to_batches = parent_dir + fetaureType + '/batchPath'\n",
    "batch_filename_arr = np.array(os.listdir(path_to_batches))\n",
    "np.random.shuffle(batch_filename_arr)\n",
    "print (batch_filename_arr)\n",
    "\n",
    "graph_dict = NNetmodel(num_output_unit=10,\n",
    "                       num_hid1_unit=100, \n",
    "                       momentum=0.9,\n",
    "                       learning_rate=0.1\n",
    "                      )\n",
    "# print (graph_dict)\n",
    "trainNetwork(graph_dict)#, path_save_model=path_save_model)\n",
    "# sys.stdout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_output_unit = 10\n",
    "no_of_labels = 10\n",
    "no_inp_unit = 1024\n",
    "no_hid_unit = 100\n",
    "learning_rate=0.1\n",
    "momentum = 0.9\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    seed = 128\n",
    "    rng = np.random.RandomState(seed)\n",
    "    tf_training_dataset = tf.placeholder(tf.float32, shape=(None, no_inp_unit))\n",
    "    tf_training_labels = tf.placeholder(tf.float32, shape=(None, no_of_labels))\n",
    "\n",
    "    weights = {\n",
    "        'input_to_hid_wghts': tf.Variable(tf.random_normal([no_inp_unit, no_hid_unit], seed=seed)),\n",
    "        'hid_to_output_wghts': tf.Variable(tf.random_normal([no_hid_unit, no_output_unit], seed=seed))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'hid_bias' : tf.Variable(tf.zeros([no_hid_unit])),\n",
    "        'output_bias' : tf.Variable(tf.zeros([no_output_unit]))\n",
    "    }\n",
    "\n",
    "    ###### Forward Propagate ######\n",
    "    # Hidden Layer\n",
    "    input_to_hid_layer = tf.matmul(tf_training_dataset, weights['input_to_hid_wghts']) + biases['hid_bias']\n",
    "    hid_layer_state = tf.sigmoid(input_to_hid_layer, name=None)\n",
    "\n",
    "    # Output Layer\n",
    "    hid_to_output_layer = tf.matmul(hid_layer_state, weights['hid_to_output_wghts']) + biases['output_bias']\n",
    "    output_layer_state = tf.nn.softmax(hid_to_output_layer, name=None)\n",
    "    error_derivative = tf.sub(output_layer_state, tf_training_labels, name=None)  # -(y - y_hat) = (y_hat - y)\n",
    "    # The above three lines of code can also be combined into one line as below.\n",
    "    loss_CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hid_to_output_layer, tf_training_labels))\n",
    "\n",
    "    \n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, \n",
    "                                            momentum, \n",
    "                                            use_locking=False, \n",
    "                                            name='Momentum', \n",
    "                                            use_nesterov=True).minimize(loss_CE)\n",
    "    # Training Prediction\n",
    "    training_prediction = output_layer_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variable Initialized successfully\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 10.507124\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 9.620941\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 8.508722\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/App-Setup/CondaENV/lib/python3.5/site-packages/ipykernel/__main__.py:19: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at epoch 0: 7.794838\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 6.984330\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 6.502948\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 6.112256\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 5.684135\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 5.508185\n",
      "Minibatch accuracy: 0.0%\n",
      "(4000, 1024)\n",
      "(4000, 10)\n",
      "Minibatch loss at epoch 0: 5.472131\n",
      "Minibatch accuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 1\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"All variable Initialized successfully\")\n",
    "    for epoch in range(epochs):\n",
    "        # Retreive mini-batches\n",
    "        for trnData, trnLabels in batch_file_iterator(path_to_batches, batch_filename_arr):\n",
    "            num_features = trnData.shape[1]*trnData.shape[2]\n",
    "            trnDataRshp, trnLabelsRshp = reshape_data(trnData, \n",
    "                                                          trnLabels, \n",
    "                                                          num_features=num_features,\n",
    "                                                          num_labels = 10\n",
    "                                                         )\n",
    "                                    \n",
    "            print (trnDataRshp.shape)\n",
    "            print (trnLabelsRshp.shape)\n",
    "#                 print (trnData.shape[1], trnData.shape[2])\n",
    "#             feed_dict = {\n",
    "#                     graph_dict['x']:trnDataRshp, \n",
    "#                     graph_dict['y']:trnLabelsRshp\n",
    "#                 }\n",
    "    \n",
    "\n",
    "                \n",
    "            feed_dict = {tf_training_dataset : trnDataRshp, tf_training_labels : trnLabelsRshp}\n",
    "            _, l, predictions = session.run([optimizer, loss_CE, training_prediction], feed_dict=feed_dict)\n",
    "#             print (predictions)\n",
    "        \n",
    "#         if (epoch % 2 == 0) or epoch == epochs-1:\n",
    "            print(\"Minibatch loss at epoch %d: %f\" % (epoch, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, trnLabelsRshp))\n",
    "#             print(\"Validation accuracy: %.1f%%\" % accuracy(crossvalid_prediction.eval(), crossvalid_labels_))\n",
    "#     print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
