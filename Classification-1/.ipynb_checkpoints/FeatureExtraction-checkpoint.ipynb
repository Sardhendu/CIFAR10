{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "import pickle\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "airplane_datapath = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/trainDataAirplane/\"\n",
    "cat_datapath = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/trainDataCat/\"\n",
    "\n",
    "featureSTDPath = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Classification-1/STD/\"\n",
    "featureEDGPath = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Classification-1/EDG/\"\n",
    "featureHOGPath = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Classification-1/HOG/\"\n",
    "\n",
    "featureSTDbatch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Classification-1/STD/batchData/\"\n",
    "featureEDGbatch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Classification-1/EDG/batchData/\"\n",
    "featureHOGbatch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Classification-1/HOG/batchData/\"\n",
    "\n",
    "ImageDir = [airplane_datapath, cat_datapath]\n",
    "\n",
    "DataDir = [featureSTDPath, featureEDGPath]#, featureHOGPath]\n",
    "\n",
    "BatchDir = [featureSTDbatch_dir, featureEDGbatch_dir, featureHOGbatch_dir]\n",
    "\n",
    "# parentPath = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HOG:\n",
    "    def __init__(self, orientations = 9, pixelsPerCell = (8, 8),cellsPerBlock = (3, 3), visualise=False, normalize = False): \n",
    "        self.orienations = orientations\n",
    "        self.pixelsPerCell = pixelsPerCell\n",
    "        self.cellsPerBlock = cellsPerBlock\n",
    "        self.visualise = visualise\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def describe(self, image):\n",
    "        hist , hog_image= hog(image,\n",
    "                            orientations = self.orienations,\n",
    "                            pixels_per_cell = self.pixelsPerCell,\n",
    "                            cells_per_block = self.cellsPerBlock,\n",
    "                            visualise= self.visualise)\n",
    "                            # normalise = self.normalize)  \n",
    "\n",
    "        return hist, hog_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standarized Feature DataSet  (5000, 3072)\n",
      "Edge Feature DataSet  (5000, 1024)\n",
      "The path already exists, you should force the dump\n",
      "The path already exists, you should force the dump\n",
      "Standarized Feature DataSet  (5000, 3072)\n",
      "Edge Feature DataSet  (5000, 1024)\n",
      "The path already exists, you should force the dump\n",
      "The path already exists, you should force the dump\n"
     ]
    }
   ],
   "source": [
    "def featureStandarize(image_pxlvals):\n",
    "    return(image_pxlvals - 255.0/2)/255.0\n",
    "\n",
    "\n",
    "def featureExtraction(pathTo_images, filenameArr, imageSize=32, mimNumImage=None, numChannels=3, HOG=None):\n",
    "    datasetSTD = np.ndarray(shape=(len(filenameArr), imageSize, imageSize, 3), dtype=np.float32)\n",
    "    datasetEDG = np.ndarray(shape=(len(filenameArr), imageSize, imageSize), dtype=np.float32)\n",
    "    \n",
    "    for numImage, image in enumerate(filenameArr):\n",
    "        imagePath = os.path.join(pathTo_images, image)\n",
    "#         print (numImage)\n",
    "        try:\n",
    "            # Get the Image\n",
    "            img = cv2.imread(imagePath)\n",
    "            # Convert the Image into Gray Scale\n",
    "            imgGS = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "            # Get normalized image\n",
    "            imgSTD = featureStandarize(imgGS)\n",
    "            \n",
    "            # Blurr the Gray Scale Image using a Gaussian Blurr\n",
    "            imgBLR = cv2.GaussianBlur(imgGS, (3,3), 3)                # The filer size is chosen to be 3 and the standard deviation for the distribution is 3\n",
    "            \n",
    "            # Detect Edges using Canny Filter\n",
    "            imgEDG = cv2.Canny(imgBLR, 20, 200)                        # The minimum threshold value chosen is 60 and the maximum threshold chosen is 150\n",
    "            \n",
    "#             print (imgSTD.shape)\n",
    "#             print(imgSTD.flatten().shape)\n",
    "            datasetSTD[numImage, :] = imgSTD\n",
    "            \n",
    "        except IOError as e:\n",
    "            print('Could not read:', image, ':', e, '- hence skipping.')\n",
    "            \n",
    "    return datasetSTD.reshape((-1,imageSize*imageSize*numChannels)), datasetEDG.reshape((-1,imageSize*imageSize))\n",
    "           \n",
    "    \n",
    "def main(forceDump=None):\n",
    "    for image_dir in ImageDir:\n",
    "        objectName = os.path.basename(os.path.normpath(image_dir))\n",
    "        filenameArr =  os.listdir(image_dir)\n",
    "        \n",
    "        datasetSTD, datasetEDG = featureExtraction(image_dir, filenameArr)\n",
    "        print ('Standarized Feature DataSet ', datasetSTD.shape)\n",
    "        print ('Edge Feature DataSet ', datasetEDG.shape)\n",
    "        \n",
    "#         for num, i in enumerate(datasetEDG):\n",
    "#             for j in i:\n",
    "#                 print (j)\n",
    "#             if num==10:\n",
    "#                 break\n",
    "                \n",
    "\n",
    "        for data_dir in DataDir:        \n",
    "            if not os.path.exists(data_dir):\n",
    "                os.makedirs(data_dir)\n",
    "                \n",
    "            featureType = os.path.basename(os.path.normpath(data_dir))    \n",
    "            fileName = data_dir+objectName+\".pickle\"\n",
    "\n",
    "            # DUMP PICKLE FILES\n",
    "            if os.path.exists(fileName) and not forceDump:\n",
    "                print ('The path already exists, you should force the dump')\n",
    "            else:\n",
    "                try:\n",
    "                    with open(fileName, 'wb') as f:\n",
    "                        if featureType=='STD':\n",
    "                            pickle.dump(datasetSTD, f, pickle.HIGHEST_PROTOCOL)\n",
    "                        elif featureType=='EDG':\n",
    "                            pickle.dump(datasetEDG, f, pickle.HIGHEST_PROTOCOL)\n",
    "                except Exception as e:\n",
    "                    print('Unable to save data to', fileName1, ':', e)\n",
    "                    \n",
    "#         break\n",
    "    \n",
    "main(forceDump=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Above Process successfully creates the data set for the features:\n",
    "    1. Standarized (Normalized) Image\n",
    "    2. Image with Edge activation pixel\n",
    "    3. Image with HOG Features (One Filter Type)\n",
    "    4. To do ..... (HOG with multple filters)\n",
    "    \n",
    "The above code is generalized to store the features as nd arrays as pickels files in the given directories.\n",
    "\n",
    "........\n",
    "\n",
    "The Below part of the code attempts to go to each directory and generate n-folds batches and store it into the respective directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys,os\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "mydir = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "sys.path.append(mydir)\n",
    "\n",
    "from DataPreparation import CreateBatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training Data set size is :  (10000, 3072)\n",
      "The training Labels size is :  (10000,)\n",
      "The test Data set size is :  (0, 3072)\n",
      "The test Labels size is :  (0,)\n",
      "Batch No:  0  : Training Batch Data Shape: (1000, 3072)\n",
      "Batch No:  0  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  1  : Training Batch Data Shape: (1000, 3072)\n",
      "Batch No:  1  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  2  : Training Batch Data Shape: (1000, 3072)\n",
      "Batch No:  2  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  3  : Training Batch Data Shape: (1000, 3072)\n",
      "Batch No:  3  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  4  : Training Batch Data Shape: (1000, 3072)\n",
      "Batch No:  4  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  5  : Training Batch Data Shape: (1000, 3072)\n",
      "Batch No:  5  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  6  : Training Batch Data Shape: (1000, 3072)\n",
      "Batch No:  6  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  7  : Training Batch Data Shape: (1000, 3072)\n",
      "Batch No:  7  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  8  : Training Batch Data Shape: (1000, 3072)\n",
      "Batch No:  8  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  9  : Training Batch Data Shape: (1000, 3072)\n",
      "Batch No:  9  : Training Batch Labels Shape : (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Create 10 Batches and stores in into the provided Batch Directory for the Strandarized Feature set of Images\n",
    "imageDim = 32*32*3\n",
    "numBatches =10\n",
    "maxNumImage = 5000\n",
    "test_percntg = 0\n",
    "\n",
    "root_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Classification-1/STD/\"\n",
    "batch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Classification-1/STD/batchData/\"\n",
    "\n",
    "obj_STD = CreateBatches(dimensions=imageDim)\n",
    "trainData, trainLabels, testLabels, _, _ = obj_STD.gen_TrainTestData(max_num_images=maxNumImage, dir_to_pickle_files=root_dir, test_percntg=test_percntg)\n",
    "\n",
    "for batchNum, (trnBatchData, trnBatchLabel) in enumerate(obj_STD.generateBatches(dataset=trainData, labels=trainLabels, numBatches=numBatches)):\n",
    "\tobj_STD.dumpBatches(trnBatchData, trnBatchLabel, batch_dir, batchNum=batchNum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 10 Batches and stores in into the provided Batch Directory for the Edge Feature set of Images\n",
    "imageDim=32*32\n",
    "numBatches =10\n",
    "maxNumImage = 5000\n",
    "test_percntg=0\n",
    "\n",
    "root_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Classification-1/EDG/\"\n",
    "batch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Classification-1/EDG/batchData/\"\n",
    "\n",
    "obj_EDG = CreateBatches(dimensions=imageDim)\n",
    "trainData, trainLabels, testLabels, _, _ = obj_EDG.gen_TrainTestData(max_num_images=maxNumImage, dir_to_pickle_files=root_dir, test_percntg=test_percntg)\n",
    "\n",
    "for batchNum, (trnBatchData, trnBatchLabel) in enumerate(obj_EDG.generateBatches(dataset=trainData, labels=trainLabels, numBatches=numBatches)):\n",
    "\tobj_EDG.dumpBatches(trnBatchData, trnBatchLabel, batch_dir, batchNum=batchNum)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
