{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## New Weights after one update: Using TensorFlow weight update\n",
    "----------\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-5).minimize(lossCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial weights initilized are: \n",
      "\n",
      "[[-2.11875319  1.11074173 -1.60225117 -1.04866672]\n",
      " [ 1.0743587  -0.37837428 -0.39874253  1.21538258]\n",
      " [ 0.70911169 -0.10536155 -1.4022162  -0.44625875]]\n",
      "\n",
      "The initial bais initilized are: \n",
      "\n",
      "[0 0 0 0]\n",
      "\n",
      "Begin training Begin training Begin training Begin training Begin training \n",
      "\n",
      "[[-2.11875176  1.1107409  -1.60225093 -1.04867029]\n",
      " [ 1.07435918 -0.37837315 -0.39874318  1.21537995]\n",
      " [ 0.70911378 -0.10536345 -1.40221572 -0.44625914]]\n",
      "\n",
      "[  6.51314224e-07  -1.33663309e-06   4.03212113e-07   2.65411586e-06]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 4\n",
    "dim = 3\n",
    "hidden_units = 4\n",
    "\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "with sess.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, dim], name=\"x\")\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"y\")\n",
    "    w = tf.Variable(initial_value=tf.random_normal(shape=[dim, hidden_units], seed=432), name=\"w\")\n",
    "    b = tf.Variable(initial_value=tf.zeros(shape=[hidden_units]), name=\"b\")\n",
    "    \n",
    "    print ('The initial weights initilized are: \\n')\n",
    "    print (tf.cast(tf.random_normal(shape=[dim, hidden_units], seed=432), dtype=tf.float32).eval())\n",
    "    print ('')\n",
    "    print ('The initial bais initilized are: \\n')\n",
    "    print (tf.cast(tf.zeros(shape=[hidden_units]), dtype=tf.int32).eval())\n",
    "    print ('')\n",
    "    \n",
    "    logits = tf.nn.tanh(tf.matmul(x, w) + b)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y, name=\"xentropy\")\n",
    "    lossCE = tf.reduce_mean(cross_entropy) \n",
    "    optimizer = tf.train.GradientDescentOptimizer(1e-5).minimize(lossCE)\n",
    "        \n",
    "    print ('Begin training Begin training Begin training Begin training Begin training ')\n",
    "    print ('')\n",
    "    \n",
    "    np.random.seed(398)\n",
    "    data = np.random.randn(batch_size, dim)\n",
    "    labels = np.random.randint(0, hidden_units, size=batch_size)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    newW, newB, _ = sess.run([w, b, optimizer], feed_dict={x:data, y:labels})\n",
    "\n",
    "    print (newW)\n",
    "    print ('')\n",
    "    print (newB)\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## New Weights after one update: Manual Update:\n",
    "* gradients = tf.gradients(lossCE, tf.trainable_variables())\n",
    "* gradients_and_vars = list(zip(gradients, tf.trainable_variables()))\n",
    "\n",
    "* for g, v in gradients_and_vars:\n",
    "   * if g is not None:\n",
    "    *    newVar = v - 1e-5*(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial weights initilized are: \n",
      "\n",
      "[[-2.11875319  1.11074173 -1.60225117 -1.04866672]\n",
      " [ 1.0743587  -0.37837428 -0.39874253  1.21538258]\n",
      " [ 0.70911169 -0.10536155 -1.4022162  -0.44625875]]\n",
      "\n",
      "The initial bais initilized are: \n",
      "\n",
      "[0 0 0 0]\n",
      "\n",
      "Begin training Begin training Begin training Begin training Begin training \n",
      "\n",
      "['w:0', 'b:0']\n",
      "Grads_and_Vars when using optimizer: \n",
      " [(array([[-0.58818352,  0.34526709, -0.08834551,  1.42918241],\n",
      "       [-0.18376471, -0.45474505,  0.26138756,  1.02784896],\n",
      "       [-0.82264823,  0.7613858 , -0.178047  ,  0.16059123]], dtype=float32), array([[-2.11875319,  1.11074173, -1.60225117, -1.04866672],\n",
      "       [ 1.0743587 , -0.37837428, -0.39874253,  1.21538258],\n",
      "       [ 0.70911169, -0.10536155, -1.4022162 , -0.44625875]], dtype=float32)), (array([-0.2605257 ,  0.53465325, -0.16128485, -1.06164634], dtype=float32), array([ 0.,  0.,  0.,  0.], dtype=float32))]\n",
      "\n",
      "Gradients_and_Vars when NOT using optimizer: \n",
      " [(array([[-0.14704588,  0.08631677, -0.02208638,  0.3572956 ],\n",
      "       [-0.04594118, -0.11368626,  0.06534689,  0.25696224],\n",
      "       [-0.20566206,  0.19034645, -0.04451175,  0.04014781]], dtype=float32), array([[-2.11875319,  1.11074173, -1.60225117, -1.04866672],\n",
      "       [ 1.0743587 , -0.37837428, -0.39874253,  1.21538258],\n",
      "       [ 0.70911169, -0.10536155, -1.4022162 , -0.44625875]], dtype=float32)), (array([-0.06513143,  0.13366331, -0.04032121, -0.26541159], dtype=float32), array([ 0.,  0.,  0.,  0.], dtype=float32))]\n",
      "\n",
      "Gradients when NOT using optimizer: \n",
      " [array([[-0.14704588,  0.08631677, -0.02208638,  0.3572956 ],\n",
      "       [-0.04594118, -0.11368626,  0.06534689,  0.25696224],\n",
      "       [-0.20566206,  0.19034645, -0.04451175,  0.04014781]], dtype=float32), array([-0.06513143,  0.13366331, -0.04032121, -0.26541159], dtype=float32)]\n",
      "\n",
      "\n",
      "Running for Variable Num  w:0\n",
      "**************** this is variable *************\n",
      "variable's shape:\n",
      "[[-2.11875319  1.11074173 -1.60225117 -1.04866672]\n",
      " [ 1.0743587  -0.37837428 -0.39874253  1.21538258]\n",
      " [ 0.70911169 -0.10536155 -1.4022162  -0.44625875]]\n",
      "**************** this is gradient *************\n",
      "gradient's shape:\n",
      "[[-0.14704588  0.08631677 -0.02208638  0.3572956 ]\n",
      " [-0.04594118 -0.11368626  0.06534689  0.25696224]\n",
      " [-0.20566206  0.19034645 -0.04451175  0.04014781]]\n",
      "\n",
      "The new Variable looks like\n",
      "[[-2.11875176  1.1107409  -1.60225093 -1.04867029]\n",
      " [ 1.07435918 -0.37837315 -0.39874318  1.21537995]\n",
      " [ 0.70911378 -0.10536345 -1.40221572 -0.44625914]]\n",
      "\n",
      "\n",
      "Running for Variable Num  b:0\n",
      "**************** this is variable *************\n",
      "variable's shape:\n",
      "[ 0.  0.  0.  0.]\n",
      "**************** this is gradient *************\n",
      "gradient's shape:\n",
      "[-0.06513143  0.13366331 -0.04032121 -0.26541159]\n",
      "\n",
      "The new Variable looks like\n",
      "[  6.51314224e-07  -1.33663309e-06   4.03212113e-07   2.65411586e-06]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 4\n",
    "dim = 3\n",
    "hidden_units = 4\n",
    "\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "with sess.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, dim], name=\"x\")\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"y\")\n",
    "    w = tf.Variable(initial_value=tf.random_normal(shape=[dim, hidden_units], seed=432), name=\"w\")\n",
    "    b = tf.Variable(initial_value=tf.zeros(shape=[hidden_units]), name=\"b\")\n",
    "    \n",
    "    print ('The initial weights initilized are: \\n')\n",
    "    print (tf.cast(tf.random_normal(shape=[dim, hidden_units], seed=432), dtype=tf.float32).eval())\n",
    "    print ('')\n",
    "    print ('The initial bais initilized are: \\n')\n",
    "    print (tf.cast(tf.zeros(shape=[hidden_units]), dtype=tf.int32).eval())\n",
    "    print ('')\n",
    "    \n",
    "    \n",
    "    logits = tf.nn.tanh(tf.matmul(x, w) + b)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y, name=\"xentropy\")\n",
    "    lossCE = tf.reduce_mean(cross_entropy)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1e-5)\n",
    "    \n",
    "    print ('Begin training Begin training Begin training Begin training Begin training ')\n",
    "    print ('')\n",
    "    \n",
    "    grads_and_vars = optimizer.compute_gradients(cross_entropy, tf.trainable_variables())\n",
    "    gradients = tf.gradients(lossCE, tf.trainable_variables())\n",
    "    gradients_and_vars = list(zip(gradients, tf.trainable_variables()))\n",
    "    print ([v.name for v in tf.trainable_variables()])\n",
    "    # generate data\n",
    "    \n",
    "    np.random.seed(398)\n",
    "    data = np.random.randn(batch_size, dim)\n",
    "    labels = np.random.randint(0, hidden_units, size=batch_size)\n",
    "    \n",
    "    # RUN SESSION:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    Grads_and_Vars, Gradients_and_Vars, Gradients = sess.run([grads_and_vars, gradients_and_vars, gradients], feed_dict={x:data, y:labels})\n",
    "    print (\"Grads_and_Vars when using optimizer: \\n\",Grads_and_Vars)\n",
    "    print ('')\n",
    "    print (\"Gradients_and_Vars when NOT using optimizer: \\n\", Gradients_and_Vars)\n",
    "    print ('')\n",
    "    print (\"Gradients when NOT using optimizer: \\n\", Gradients)\n",
    "    print ('')\n",
    "    print ('')\n",
    "\n",
    "    for varNum, (g, v) in enumerate(Gradients_and_Vars):\n",
    "#         print (varNum)\n",
    "        print ('Running for Variable Num ', variableNames[varNum])\n",
    "        if g is not None:\n",
    "            print (\"**************** this is variable *************\")\n",
    "            print (\"variable's shape:\")\n",
    "            print (v)\n",
    "            print (\"**************** this is gradient *************\")\n",
    "            print (\"gradient's shape:\",)\n",
    "            print (g)\n",
    "            \n",
    "            print ('')\n",
    "            print ('The new Variable looks like')\n",
    "            newVar = v - 1e-5*(g)\n",
    "            print (newVar)\n",
    "            print('')\n",
    "            print ('')\n",
    "\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Experimenting the above situation with 2 layers:\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## New Weights after one update: Using TensorFlow weight update\n",
    "----------\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-5).minimize(lossCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial weights (w1) initilized are: \n",
      "\n",
      "[[-2.11875319  1.11074173 -1.60225117 -1.04866672]\n",
      " [ 1.0743587  -0.37837428 -0.39874253  1.21538258]\n",
      " [ 0.70911169 -0.10536155 -1.4022162  -0.44625875]]\n",
      "\n",
      "The initial bais (b1) initilized are: \n",
      "\n",
      "[0 0 0 0]\n",
      "\n",
      "The initial weights (w2) initilized are: \n",
      "\n",
      "[[-2.11875319  1.11074173 -1.60225117 -1.04866672]\n",
      " [ 1.0743587  -0.37837428 -0.39874253  1.21538258]\n",
      " [ 0.70911169 -0.10536155 -1.4022162  -0.44625875]]\n",
      "\n",
      "The initial bais (b1) initilized are: \n",
      "\n",
      "[0 0 0 0]\n",
      "\n",
      "Begin training Begin training Begin training Begin training Begin training \n",
      "\n",
      "[[-2.11875319  1.11074126 -1.60225117 -1.0486691 ]\n",
      " [ 1.07435882 -0.37837431 -0.39874256  1.21538115]\n",
      " [ 0.70911139 -0.10536185 -1.4022162  -0.44625914]]\n",
      "\n",
      "[ -3.27776860e-07   1.07476318e-07   2.79387002e-08   1.60490333e-06]\n",
      "\n",
      "[[-2.11875343  1.11074054 -1.60225117 -1.04866552]\n",
      " [ 1.07435882 -0.37837321 -0.39874253  1.2153815 ]\n",
      " [ 0.70911151 -0.10536274 -1.4022162  -0.44625753]\n",
      " [-0.02040439 -1.24266374 -0.77951401  1.5199616 ]]\n",
      "\n",
      "[  9.54884740e-08  -2.44732610e-06  -5.94951999e-09  -7.12481807e-08]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 4\n",
    "dim = 3\n",
    "hidden_units1 = 4\n",
    "hidden_units2 = 4\n",
    "\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "with sess.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, dim], name=\"x\")\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"y\")\n",
    "    w1 = tf.Variable(initial_value=tf.random_normal(shape=[dim, hidden_units1], seed=432), name=\"w1\")\n",
    "    b1 = tf.Variable(initial_value=tf.zeros(shape=[hidden_units1]), name=\"b1\")\n",
    "    w2 = tf.Variable(initial_value=tf.random_normal(shape=[hidden_units1, hidden_units2], seed=989), name=\"w2\")\n",
    "    b2 = tf.Variable(initial_value=tf.zeros(shape=[hidden_units2]), name=\"b2\") \n",
    "    \n",
    "    print ('The initial weights (w1) initilized are: \\n')\n",
    "    print (tf.cast(tf.random_normal(shape=[dim, hidden_units], seed=432), dtype=tf.float32).eval())\n",
    "    print ('')\n",
    "    print ('The initial bais (b1) initilized are: \\n')\n",
    "    print (tf.cast(tf.zeros(shape=[hidden_units]), dtype=tf.int32).eval())\n",
    "    print ('')\n",
    "    print ('The initial weights (w2) initilized are: \\n')\n",
    "    print (tf.cast(tf.random_normal(shape=[dim, hidden_units], seed=989), dtype=tf.float32).eval())\n",
    "    print ('')\n",
    "    print ('The initial bais (b1) initilized are: \\n')\n",
    "    print (tf.cast(tf.zeros(shape=[hidden_units]), dtype=tf.int32).eval())\n",
    "    print ('')\n",
    "    \n",
    "    \n",
    "    logits1 = tf.nn.tanh(tf.matmul(x, w1) + b1)\n",
    "    logits2 = tf.nn.tanh(tf.matmul(logits1, w2) + b2)\n",
    "    \n",
    "    ################\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits2, labels=y, name=\"xentropy\")\n",
    "    lossCE = tf.reduce_mean(cross_entropy) \n",
    "    optimizer = tf.train.GradientDescentOptimizer(1e-5).minimize(lossCE)\n",
    "    ################\n",
    "    \n",
    "    print ('Begin training Begin training Begin training Begin training Begin training ')\n",
    "    print ('')\n",
    "    \n",
    "    np.random.seed(398)\n",
    "    data = np.random.randn(batch_size, dim)\n",
    "    labels = np.random.randint(0, hidden_units2, size=batch_size)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    newW1, newB1, newW2, newB2, _ = sess.run([w1, b1, w2, b2, optimizer], feed_dict={x:data, y:labels})\n",
    "\n",
    "    print (newW1)\n",
    "    print ('')\n",
    "    print (newB1)\n",
    "    print ('')\n",
    "    print (newW2)\n",
    "    print ('')\n",
    "    print (newB2)\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial weights (w1) initilized are: \n",
      "\n",
      "[[-2.11875319  1.11074173 -1.60225117 -1.04866672]\n",
      " [ 1.0743587  -0.37837428 -0.39874253  1.21538258]\n",
      " [ 0.70911169 -0.10536155 -1.4022162  -0.44625875]]\n",
      "\n",
      "The initial bais (b1) initilized are: \n",
      "\n",
      "[0 0 0 0]\n",
      "\n",
      "The initial weights (w2) initilized are: \n",
      "\n",
      "[[-2.41577721 -0.75244576  2.13699627  0.52897286]\n",
      " [-0.88196027 -1.1298064  -0.1327185   1.08851182]\n",
      " [-1.2091223  -0.32934931  0.72396326 -0.67482239]]\n",
      "\n",
      "The initial bais (b1) initilized are: \n",
      "\n",
      "[0 0 0 0]\n",
      "\n",
      "Begin training Begin training Begin training Begin training Begin training \n",
      "\n",
      "Grads_and_Vars when using optimizer: \n",
      " [(array([[ 0.12204197,  0.44748574, -0.02048684,  0.27312103],\n",
      "       [ 0.08219481,  0.30057651,  0.03871645,  0.2215021 ],\n",
      "       [ 0.06438579,  0.06360757, -0.02989684,  0.00324626]], dtype=float32), array([[-2.11875319,  1.11074173, -1.60225117, -1.04866672],\n",
      "       [ 1.0743587 , -0.37837428, -0.39874253,  1.21538258],\n",
      "       [ 0.70911169, -0.10536155, -1.4022162 , -0.44625875]], dtype=float32)), (array([-0.04621922, -0.31476748, -0.02209643, -0.23134622], dtype=float32), array([ 0.,  0.,  0.,  0.], dtype=float32)), (array([[ 0.02600339,  0.03594229,  0.02065514, -0.85354167],\n",
      "       [-0.03080492, -0.01701269, -0.01701956,  0.75395048],\n",
      "       [ 0.03670534,  0.02967389,  0.02104382, -0.90754771],\n",
      "       [ 0.03265493, -0.15582368,  0.01121951, -0.51058811]], dtype=float32), array([[-2.41577721, -0.75244576,  2.13699627,  0.52897286],\n",
      "       [-0.88196027, -1.1298064 , -0.1327185 ,  1.08851182],\n",
      "       [-1.2091223 , -0.32934931,  0.72396326, -0.67482239],\n",
      "       [ 0.308617  , -1.04130292,  0.30698502,  0.04391456]], dtype=float32)), (array([-0.0276332 ,  0.45890504,  0.00762339, -0.43928206], dtype=float32), array([ 0.,  0.,  0.,  0.], dtype=float32))]\n",
      "\n",
      "Gradients_and_Vars when NOT using optimizer: \n",
      " [(array([[ 0.03051049,  0.11187144, -0.00512171,  0.06828026],\n",
      "       [ 0.0205487 ,  0.07514413,  0.00967911,  0.05537552],\n",
      "       [ 0.01609645,  0.01590189, -0.00747421,  0.00081157]], dtype=float32), array([[-2.11875319,  1.11074173, -1.60225117, -1.04866672],\n",
      "       [ 1.0743587 , -0.37837428, -0.39874253,  1.21538258],\n",
      "       [ 0.70911169, -0.10536155, -1.4022162 , -0.44625875]], dtype=float32)), (array([-0.01155481, -0.07869187, -0.00552411, -0.05783655], dtype=float32), array([ 0.,  0.,  0.,  0.], dtype=float32)), (array([[ 0.00650085,  0.00898557,  0.00516379, -0.21338542],\n",
      "       [-0.00770123, -0.00425317, -0.00425489,  0.18848762],\n",
      "       [ 0.00917633,  0.00741847,  0.00526096, -0.22688693],\n",
      "       [ 0.00816373, -0.03895592,  0.00280488, -0.12764703]], dtype=float32), array([[-2.41577721, -0.75244576,  2.13699627,  0.52897286],\n",
      "       [-0.88196027, -1.1298064 , -0.1327185 ,  1.08851182],\n",
      "       [-1.2091223 , -0.32934931,  0.72396326, -0.67482239],\n",
      "       [ 0.308617  , -1.04130292,  0.30698502,  0.04391456]], dtype=float32)), (array([-0.0069083 ,  0.11472626,  0.00190585, -0.10982051], dtype=float32), array([ 0.,  0.,  0.,  0.], dtype=float32))]\n",
      "\n",
      "Gradients when NOT using optimizer: \n",
      " [array([[ 0.03051049,  0.11187144, -0.00512171,  0.06828026],\n",
      "       [ 0.0205487 ,  0.07514413,  0.00967911,  0.05537552],\n",
      "       [ 0.01609645,  0.01590189, -0.00747421,  0.00081157]], dtype=float32), array([-0.01155481, -0.07869187, -0.00552411, -0.05783655], dtype=float32), array([[ 0.00650085,  0.00898557,  0.00516379, -0.21338542],\n",
      "       [-0.00770123, -0.00425317, -0.00425489,  0.18848762],\n",
      "       [ 0.00917633,  0.00741847,  0.00526096, -0.22688693],\n",
      "       [ 0.00816373, -0.03895592,  0.00280488, -0.12764703]], dtype=float32), array([-0.0069083 ,  0.11472626,  0.00190585, -0.10982051], dtype=float32)]\n",
      "\n",
      "\n",
      "Running for Variable Num  w1:0\n",
      "\n",
      "The new Variable looks like\n",
      "[[-2.11875343  1.11074066 -1.60225117 -1.04866743]\n",
      " [ 1.07435846 -0.37837502 -0.39874262  1.21538198]\n",
      " [ 0.70911151 -0.10536171 -1.40221608 -0.44625875]]\n",
      "\n",
      "\n",
      "Running for Variable Num  b1:0\n",
      "\n",
      "The new Variable looks like\n",
      "[  1.15548055e-07   7.86918690e-07   5.52410668e-08   5.78365530e-07]\n",
      "\n",
      "\n",
      "Running for Variable Num  w2:0\n",
      "\n",
      "The new Variable looks like\n",
      "[[-2.41577721 -0.75244588  2.13699627  0.52897501]\n",
      " [-0.88196021 -1.1298064  -0.13271846  1.08850992]\n",
      " [-1.20912242 -0.32934937  0.7239632  -0.67482013]\n",
      " [ 0.30861691 -1.04130256  0.30698499  0.04391584]]\n",
      "\n",
      "\n",
      "Running for Variable Num  b2:0\n",
      "\n",
      "The new Variable looks like\n",
      "[  6.90829935e-08  -1.14726254e-06  -1.90584810e-08   1.09820508e-06]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 4\n",
    "dim = 3\n",
    "hidden_units1 = 4\n",
    "hidden_units2 = 4\n",
    "\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "with sess.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, dim], name=\"x\")\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"y\")\n",
    "    w1 = tf.Variable(initial_value=tf.random_normal(shape=[dim, hidden_units1], seed=432), name=\"w1\")\n",
    "    b1 = tf.Variable(initial_value=tf.zeros(shape=[hidden_units1]), name=\"b1\")\n",
    "    w2 = tf.Variable(initial_value=tf.random_normal(shape=[hidden_units1, hidden_units2], seed=989), name=\"w2\")\n",
    "    b2 = tf.Variable(initial_value=tf.zeros(shape=[hidden_units2]), name=\"b2\") \n",
    "    \n",
    "    print ('The initial weights (w1) initilized are: \\n')\n",
    "    print (tf.cast(tf.random_normal(shape=[dim, hidden_units], seed=432), dtype=tf.float32).eval())\n",
    "    print ('')\n",
    "    print ('The initial bais (b1) initilized are: \\n')\n",
    "    print (tf.cast(tf.zeros(shape=[hidden_units]), dtype=tf.int32).eval())\n",
    "    print ('')\n",
    "    print ('The initial weights (w2) initilized are: \\n')\n",
    "    print (tf.cast(tf.random_normal(shape=[dim, hidden_units], seed=989), dtype=tf.float32).eval())\n",
    "    print ('')\n",
    "    print ('The initial bais (b1) initilized are: \\n')\n",
    "    print (tf.cast(tf.zeros(shape=[hidden_units]), dtype=tf.int32).eval())\n",
    "    print ('')\n",
    "    \n",
    "    \n",
    "    logits1 = tf.nn.tanh(tf.matmul(x, w1) + b1)\n",
    "    logits2 = tf.nn.tanh(tf.matmul(logits1, w2) + b2)\n",
    "    \n",
    "    \n",
    "    #################\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits2, labels=y, name=\"xentropy\")\n",
    "    lossCE = tf.reduce_mean(cross_entropy)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1e-5)\n",
    "    #################\n",
    "    \n",
    "    print ('Begin training Begin training Begin training Begin training Begin training ')\n",
    "    print ('')\n",
    "    \n",
    "    # Fetching variables to manually caluculate the new weights\n",
    "    grads_and_vars = optimizer.compute_gradients(cross_entropy, tf.trainable_variables())\n",
    "    gradients = tf.gradients(lossCE, tf.trainable_variables())\n",
    "    gradients_and_vars = list(zip(gradients, tf.trainable_variables()))\n",
    "    variableNames = [v.name for v in tf.trainable_variables()]\n",
    "    # generate data\n",
    "    \n",
    "    np.random.seed(398)\n",
    "    data = np.random.randn(batch_size, dim)\n",
    "    labels = np.random.randint(0, hidden_units2, size=batch_size)\n",
    "    \n",
    "    # RUN SESSION:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    Grads_and_Vars, Gradients_and_Vars, Gradients = sess.run([grads_and_vars, gradients_and_vars, gradients], feed_dict={x:data, y:labels})\n",
    "    print (\"Grads_and_Vars when using optimizer: \\n\",Grads_and_Vars)\n",
    "    print ('')\n",
    "    print (\"Gradients_and_Vars when NOT using optimizer: \\n\", Gradients_and_Vars)\n",
    "    print ('')\n",
    "    print (\"Gradients when NOT using optimizer: \\n\", Gradients)\n",
    "    print ('')\n",
    "    print ('')\n",
    "\n",
    "    for varNum, (g, v) in enumerate(Gradients_and_Vars):\n",
    "#         print (varNum)\n",
    "        print ('Running for Variable Num ', variableNames[varNum])\n",
    "        if g is not None:\n",
    "#             print (\"**************** this is variable *************\")\n",
    "#             print (\"variable's shape:\")\n",
    "#             print (v)\n",
    "#             print (\"**************** this is gradient *************\")\n",
    "#             print (\"gradient's shape:\",)\n",
    "#             print (g)\n",
    "            \n",
    "            print ('')\n",
    "            print ('The new Variable looks like')\n",
    "            newVar = v - 1e-5*(g)\n",
    "            print (newVar)\n",
    "            print('')\n",
    "            print ('')\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Try to train and Cross validate without creating a seperate crossvalidation graph:\n",
    "\n",
    "The objective of this section is to just check if the following approach has the same weight for the 1 iteration of training and the crossvalidation stage. If that is the case then we know that the weights are not changed or updted because of the crossvalidation data.\n",
    "\n",
    "We also proceed the above state by optimizing with the function with cross validation data. If the weith changes than we know that infact the first stage was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training Begin training Begin training Begin training Begin training \n",
      "\n",
      "############### Check the weights after the training batch (compare the results to previous example): \n",
      "\n",
      "The loss after one iteration is:  2.11358\n",
      "\n",
      "The variable list is:  ['Layer1/w1:0', 'Layer1/b1:0', 'Layer2/w2:0', 'Layer2/b2:0']\n",
      "\n",
      "The layer 1 weight is: \n",
      " [array([[-2.11875343,  1.11074066, -1.60225117, -1.04866743],\n",
      "       [ 1.07435846, -0.37837502, -0.39874262,  1.21538198],\n",
      "       [ 0.70911151, -0.10536171, -1.40221608, -0.44625875]], dtype=float32)]\n",
      "\n",
      "The layer 1 bias is: \n",
      " [array([  1.15548055e-07,   7.86918690e-07,   5.52410668e-08,\n",
      "         5.78365530e-07], dtype=float32)]\n",
      "\n",
      "The layer 2 weight is: \n",
      " [array([[-2.41577721, -0.75244588,  2.13699627,  0.52897501],\n",
      "       [-0.88196021, -1.1298064 , -0.13271846,  1.08850992],\n",
      "       [-1.20912242, -0.32934937,  0.7239632 , -0.67482013],\n",
      "       [ 0.30861691, -1.04130256,  0.30698499,  0.04391584]], dtype=float32)]\n",
      "\n",
      "The layer 2 bias is: \n",
      " [array([  6.90829935e-08,  -1.14726254e-06,  -1.90584810e-08,\n",
      "         1.09820508e-06], dtype=float32)]\n",
      "\n",
      "\n",
      "############### Check if the weights are the same after the validation batch (compare the results to weights after training example): \n",
      "\n",
      "The loss after validation iteration is:  2.11358\n",
      "\n",
      "The variable list is:  ['Layer1/w1:0', 'Layer1/b1:0', 'Layer2/w2:0', 'Layer2/b2:0']\n",
      "\n",
      "The layer 1 weight is: \n",
      " [array([[-2.11875343,  1.11074066, -1.60225117, -1.04866743],\n",
      "       [ 1.07435846, -0.37837502, -0.39874262,  1.21538198],\n",
      "       [ 0.70911151, -0.10536171, -1.40221608, -0.44625875]], dtype=float32)]\n",
      "\n",
      "The layer 1 bias is: \n",
      " [array([  1.15548055e-07,   7.86918690e-07,   5.52410668e-08,\n",
      "         5.78365530e-07], dtype=float32)]\n",
      "\n",
      "The layer 2 weight is: \n",
      " [array([[-2.41577721, -0.75244588,  2.13699627,  0.52897501],\n",
      "       [-0.88196021, -1.1298064 , -0.13271846,  1.08850992],\n",
      "       [-1.20912242, -0.32934937,  0.7239632 , -0.67482013],\n",
      "       [ 0.30861691, -1.04130256,  0.30698499,  0.04391584]], dtype=float32)]\n",
      "\n",
      "The layer 2 bias is: \n",
      " [array([  6.90829935e-08,  -1.14726254e-06,  -1.90584810e-08,\n",
      "         1.09820508e-06], dtype=float32)]\n",
      "############### Check if the weights change after using the optimizer with validation dataset : \n",
      "\n",
      "The loss after validation iteration is:  2.26603\n",
      "\n",
      "The variable list is:  ['Layer1/w1:0', 'Layer1/b1:0', 'Layer2/w2:0', 'Layer2/b2:0']\n",
      "\n",
      "The layer 1 weight is: \n",
      " [array([[-2.11875296,  1.11074114, -1.6022507 , -1.04866672],\n",
      "       [ 1.07435822, -0.37837639, -0.39874244,  1.21538115],\n",
      "       [ 0.70911211, -0.10536141, -1.40221667, -0.44625884]], dtype=float32)]\n",
      "\n",
      "The layer 1 bias is: \n",
      " [array([ -4.02032185e-07,  -2.14392799e-06,   4.44028501e-07,\n",
      "        -1.36022834e-06], dtype=float32)]\n",
      "\n",
      "The layer 2 weight is: \n",
      " [array([[-2.41577649, -0.75244486,  2.13699627,  0.52897495],\n",
      "       [-0.88196051, -1.12980747, -0.13271837,  1.08850992],\n",
      "       [-1.20912278, -0.32934901,  0.72396302, -0.67481983],\n",
      "       [ 0.30861658, -1.04130054,  0.30698469,  0.04391532]], dtype=float32)]\n",
      "\n",
      "The layer 2 bias is: \n",
      " [array([ -6.69981205e-07,   1.44030105e-06,  -4.06810130e-07,\n",
      "        -2.74756985e-07], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "dim = 3\n",
    "hidden_units1 = 4\n",
    "hidden_units2 = 4\n",
    "\n",
    "def linearActivation1(scope, x):\n",
    "    with tf.variable_scope(scope):\n",
    "        w = tf.get_variable(dtype=tf.float32, initializer=tf.random_normal_initializer(seed=432), shape=[dim, hidden_units1], name=\"w1\")\n",
    "        b = tf.get_variable(dtype = tf.float32, initializer=tf.zeros(dtype=tf.float32, shape=[hidden_units1]), name=\"b1\" )\n",
    "        \n",
    "        return tf.nn.tanh(tf.matmul(x, w) + b)\n",
    "    \n",
    "def linearActivation2(scope, x):\n",
    "    with tf.variable_scope(scope):\n",
    "        w = tf.get_variable(dtype=tf.float32, initializer=tf.random_normal_initializer(seed=989), shape=[hidden_units1, hidden_units2], name=\"w2\")\n",
    "        b = tf.get_variable(dtype = tf.float32, initializer=tf.zeros(dtype=tf.float32, shape=[hidden_units2]), name=\"b2\" )\n",
    "        \n",
    "        return tf.nn.tanh(tf.matmul(x, w) + b)\n",
    "    \n",
    "    \n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, dim], name=\"x\")\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"y\")\n",
    "    \n",
    "    logits1 = linearActivation1(\"Layer1\", x)\n",
    "#     print (logits1.get_shape().as_list())\n",
    "    logits2 = linearActivation2(\"Layer2\", logits1)\n",
    "    \n",
    "    \n",
    "    #################\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits2, labels=y, name=\"xentropy\")\n",
    "    lossCE = tf.reduce_mean(cross_entropy) \n",
    "    optimizer = tf.train.GradientDescentOptimizer(1e-5).minimize(lossCE)\n",
    "        \n",
    "    #################\n",
    "    \n",
    "    print ('Begin training Begin training Begin training Begin training Begin training ')\n",
    "    print ('')\n",
    "    \n",
    "    np.random.seed(398)\n",
    "    trainData = np.random.randn(batch_size, dim)\n",
    "    trainLabels = np.random.randint(0, hidden_units2, size=batch_size)\n",
    "\n",
    "    \n",
    "    # RUN SESSION:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # When Training\n",
    "    _, loss, cent = sess.run([optimizer, lossCE, cross_entropy], feed_dict={x:trainData, y:trainLabels})\n",
    "    \n",
    "\n",
    "    print ('############### Check the weights after the training batch (compare the results to previous example): \\n')\n",
    "    print ('The loss after one iteration is: ', loss)\n",
    "    print ('')\n",
    "    variableList = [v.name for v in tf.trainable_variables()]\n",
    "    print ('The variable list is: ', variableList)\n",
    "    print ('')\n",
    "    print ('The layer 1 weight is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer1/w1:0']))\n",
    "    print ('')\n",
    "    print ('The layer 1 bias is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer1/b1:0']))\n",
    "    print ('')\n",
    "    print ('The layer 2 weight is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer2/w2:0']))\n",
    "    print ('')\n",
    "    print ('The layer 2 bias is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer2/b2:0']))\n",
    "    print ('')\n",
    "    print ('')\n",
    "    \n",
    "    # When Cross Validation (Since we dont initiate the computation graph for optimization, the weights dont change)\n",
    "    np.random.seed(778)\n",
    "    validData = np.random.randn(batch_size, dim)\n",
    "    validLabel = np.random.randint(0, hidden_units2, size=batch_size)\n",
    "    \n",
    "    cent = sess.run([ cross_entropy], feed_dict={x:validData, y:validLabel})\n",
    "    \n",
    "\n",
    "    print ('############### Check if the weights are the same after the validation batch (compare the results to weights after training example): \\n')\n",
    "    print ('The loss after validation iteration is: ', loss)\n",
    "    print ('')\n",
    "    variableList = [v.name for v in tf.trainable_variables()]\n",
    "    print ('The variable list is: ', variableList)\n",
    "    print ('')\n",
    "    print ('The layer 1 weight is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer1/w1:0']))\n",
    "    print ('')\n",
    "    print ('The layer 1 bias is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer1/b1:0']))\n",
    "    print ('')\n",
    "    print ('The layer 2 weight is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer2/w2:0']))\n",
    "    print ('')\n",
    "    print ('The layer 2 bias is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer2/b2:0']))\n",
    "    \n",
    "    # When Cross Validation with optimization and loss (This should change the weights)\n",
    "\n",
    "    _, loss, cent = sess.run([optimizer, lossCE, cross_entropy], feed_dict={x:validData, y:validLabel})\n",
    "    \n",
    "    print ('############### Check if the weights change after using the optimizer with validation dataset : \\n')\n",
    "    print ('The loss after validation iteration is: ', loss)\n",
    "    print ('')\n",
    "    variableList = [v.name for v in tf.trainable_variables()]\n",
    "    print ('The variable list is: ', variableList)\n",
    "    print ('')\n",
    "    print ('The layer 1 weight is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer1/w1:0']))\n",
    "    print ('')\n",
    "    print ('The layer 1 bias is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer1/b1:0']))\n",
    "    print ('')\n",
    "    print ('The layer 2 weight is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer2/w2:0']))\n",
    "    print ('')\n",
    "    print ('The layer 2 bias is: \\n', sess.run([v for v in tf.trainable_variables() if v.name == 'Layer2/b2:0']))\n",
    "sess.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Learning Rate Decay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "Input Data:  [[ 0.73344636 -1.25675774 -0.4910166 ]\n",
      " [ 0.44920754  1.30595517 -2.11853456]\n",
      " [-0.28212729 -0.69542331 -0.27580655]\n",
      " [ 0.31027448  0.34955853 -0.5828709 ]]\n",
      "Labels : [[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]]\n",
      "linOUT :  [[-1.87049329  3.32683158]\n",
      " [ 2.61160016  1.85904086]\n",
      " [-1.04206681  1.38684106]\n",
      " [ 0.7326476   2.22365403]]\n",
      "nonlinOUT :  [[ 0.          3.32683158]\n",
      " [ 2.61160016  1.85904086]\n",
      " [ 0.          1.38684106]\n",
      " [ 0.7326476   2.22365403]]\n",
      "loss :  0.746706\n",
      "learning rate :  0.1\n",
      "global_step :  1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "ops.reset_default_graph()\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.1\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, \n",
    "                                           global_step,    # Used for decay computation\n",
    "                                           1,              # Decay steps\n",
    "                                           0.96,           # Decay rate\n",
    "                                           staircase=True) # Will decay the learning rate in discrete interval\n",
    "\n",
    "# Creating a 1 layer Neural Network, to test the learning Rate\n",
    "# define some Random weights\n",
    "numFeatures = 3\n",
    "numOut = 2\n",
    "numLabels = 2\n",
    "w1 = tf.Variable(tf.random_normal([numFeatures,numOut],seed=984))\n",
    "b1 = tf.Variable(tf.random_normal([numOut],seed=984))\n",
    "# print (w1.get_shape().as_list(), b1.get_shape().as_list())\n",
    "# def network():\n",
    "xIN = tf.placeholder(tf.float32, [None, numFeatures])\n",
    "yIN = tf.placeholder(tf.float32, [None, numLabels])\n",
    "linOUT = tf.matmul(xIN, w1) + b1\n",
    "nonlinOUT = tf.nn.relu(linOUT)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=nonlinOUT, labels=yIN))\n",
    "optimizer = (\n",
    "    tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    .minimize(loss, global_step=global_step)\n",
    ")\n",
    "    \n",
    "# Here below we build a simple 1 layer network\n",
    "random.seed(9274)\n",
    "trainData = np.random.randn(36,numFeatures)\n",
    "trainLabelVec = np.concatenate((np.ones(18, dtype=int), np.zeros(18, dtype=int)))\n",
    "random.shuffle(trainLabelVec)\n",
    "# COnverting Label vector into Label one hor vector\n",
    "trainLabels = np.eye(numLabels)[trainLabelVec]\n",
    "\n",
    "batchSize = 4\n",
    "startVal = 0\n",
    "endVal = batchSize\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    while endVal == len(trainData):\n",
    "#         print (data, '\\n', labels)\n",
    "#         print (data.shape)\n",
    "        feedDict = {xIN:trainData[startVal:endVal], yIN:trainLabels[startVal:endVal]}\n",
    "        x_in, y_in, lin_out, nlin_out, ls, _, l_rate, g_step = sess.run([xIN, yIN, linOUT, \n",
    "                                                                         nonlinOUT, loss, optimizer, \n",
    "                                                                         learning_rate, global_step], feed_dict=feedDict)\n",
    "        startVal += batchSize\n",
    "        endVal += batchSize\n",
    "        print ('##########################################')\n",
    "        print ('Input Data: ', x_in)\n",
    "        print ('Labels :', y_in )\n",
    "        print ('linOUT : ', lin_out)\n",
    "        print ('nonlinOUT : ', nlin_out)\n",
    "        print ('loss : ', ls)\n",
    "        print ('learning rate : ', l_rate)\n",
    "        print ('global_step : ', g_step)\n",
    "        print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'Variable/initial_value' type=Const>,\n",
       " <tf.Operation 'Variable' type=VariableV2>,\n",
       " <tf.Operation 'Variable/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable/read' type=Identity>,\n",
       " <tf.Operation 'ExponentialDecay/learning_rate' type=Const>,\n",
       " <tf.Operation 'ExponentialDecay/Cast' type=Cast>,\n",
       " <tf.Operation 'ExponentialDecay/Cast_1/x' type=Const>,\n",
       " <tf.Operation 'ExponentialDecay/Cast_1' type=Cast>,\n",
       " <tf.Operation 'ExponentialDecay/Cast_2/x' type=Const>,\n",
       " <tf.Operation 'ExponentialDecay/truediv' type=RealDiv>,\n",
       " <tf.Operation 'ExponentialDecay/Floor' type=Floor>,\n",
       " <tf.Operation 'ExponentialDecay/Pow' type=Pow>,\n",
       " <tf.Operation 'ExponentialDecay' type=Mul>,\n",
       " <tf.Operation 'random_normal/shape' type=Const>,\n",
       " <tf.Operation 'random_normal/mean' type=Const>,\n",
       " <tf.Operation 'random_normal/stddev' type=Const>,\n",
       " <tf.Operation 'random_normal/RandomStandardNormal' type=RandomStandardNormal>,\n",
       " <tf.Operation 'random_normal/mul' type=Mul>,\n",
       " <tf.Operation 'random_normal' type=Add>,\n",
       " <tf.Operation 'Variable_1' type=VariableV2>,\n",
       " <tf.Operation 'Variable_1/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_1/read' type=Identity>,\n",
       " <tf.Operation 'random_normal_1/shape' type=Const>,\n",
       " <tf.Operation 'random_normal_1/mean' type=Const>,\n",
       " <tf.Operation 'random_normal_1/stddev' type=Const>,\n",
       " <tf.Operation 'random_normal_1/RandomStandardNormal' type=RandomStandardNormal>,\n",
       " <tf.Operation 'random_normal_1/mul' type=Mul>,\n",
       " <tf.Operation 'random_normal_1' type=Add>,\n",
       " <tf.Operation 'Variable_2' type=VariableV2>,\n",
       " <tf.Operation 'Variable_2/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_2/read' type=Identity>,\n",
       " <tf.Operation 'Placeholder' type=Placeholder>,\n",
       " <tf.Operation 'Placeholder_1' type=Placeholder>,\n",
       " <tf.Operation 'MatMul' type=MatMul>,\n",
       " <tf.Operation 'add' type=Add>,\n",
       " <tf.Operation 'Relu' type=Relu>,\n",
       " <tf.Operation 'Rank' type=Const>,\n",
       " <tf.Operation 'Shape' type=Shape>,\n",
       " <tf.Operation 'Rank_1' type=Const>,\n",
       " <tf.Operation 'Shape_1' type=Shape>,\n",
       " <tf.Operation 'Sub/y' type=Const>,\n",
       " <tf.Operation 'Sub' type=Sub>,\n",
       " <tf.Operation 'Slice/begin' type=Pack>,\n",
       " <tf.Operation 'Slice/size' type=Const>,\n",
       " <tf.Operation 'Slice' type=Slice>,\n",
       " <tf.Operation 'concat/values_0' type=Const>,\n",
       " <tf.Operation 'concat/axis' type=Const>,\n",
       " <tf.Operation 'concat' type=ConcatV2>,\n",
       " <tf.Operation 'Reshape' type=Reshape>,\n",
       " <tf.Operation 'Rank_2' type=Const>,\n",
       " <tf.Operation 'Shape_2' type=Shape>,\n",
       " <tf.Operation 'Sub_1/y' type=Const>,\n",
       " <tf.Operation 'Sub_1' type=Sub>,\n",
       " <tf.Operation 'Slice_1/begin' type=Pack>,\n",
       " <tf.Operation 'Slice_1/size' type=Const>,\n",
       " <tf.Operation 'Slice_1' type=Slice>,\n",
       " <tf.Operation 'concat_1/values_0' type=Const>,\n",
       " <tf.Operation 'concat_1/axis' type=Const>,\n",
       " <tf.Operation 'concat_1' type=ConcatV2>,\n",
       " <tf.Operation 'Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'SoftmaxCrossEntropyWithLogits' type=SoftmaxCrossEntropyWithLogits>,\n",
       " <tf.Operation 'Sub_2/y' type=Const>,\n",
       " <tf.Operation 'Sub_2' type=Sub>,\n",
       " <tf.Operation 'Slice_2/begin' type=Const>,\n",
       " <tf.Operation 'Slice_2/size' type=Pack>,\n",
       " <tf.Operation 'Slice_2' type=Slice>,\n",
       " <tf.Operation 'Reshape_2' type=Reshape>,\n",
       " <tf.Operation 'Const' type=Const>,\n",
       " <tf.Operation 'Mean' type=Mean>,\n",
       " <tf.Operation 'gradients/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/Const' type=Const>,\n",
       " <tf.Operation 'gradients/Fill' type=Fill>,\n",
       " <tf.Operation 'gradients/Mean_grad/Reshape/shape' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/Mean_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/Mean_grad/Tile' type=Tile>,\n",
       " <tf.Operation 'gradients/Mean_grad/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/Mean_grad/Shape_2' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Const' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Prod' type=Prod>,\n",
       " <tf.Operation 'gradients/Mean_grad/Const_1' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Prod_1' type=Prod>,\n",
       " <tf.Operation 'gradients/Mean_grad/Maximum/y' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Maximum' type=Maximum>,\n",
       " <tf.Operation 'gradients/Mean_grad/floordiv' type=FloorDiv>,\n",
       " <tf.Operation 'gradients/Mean_grad/Cast' type=Cast>,\n",
       " <tf.Operation 'gradients/Mean_grad/truediv' type=RealDiv>,\n",
       " <tf.Operation 'gradients/Reshape_2_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/Reshape_2_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/zeros_like' type=ZerosLike>,\n",
       " <tf.Operation 'gradients/SoftmaxCrossEntropyWithLogits_grad/PreventGradient' type=PreventGradient>,\n",
       " <tf.Operation 'gradients/SoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim' type=Const>,\n",
       " <tf.Operation 'gradients/SoftmaxCrossEntropyWithLogits_grad/ExpandDims' type=ExpandDims>,\n",
       " <tf.Operation 'gradients/SoftmaxCrossEntropyWithLogits_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/Reshape_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/Reshape_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/Relu_grad/ReluGrad' type=ReluGrad>,\n",
       " <tf.Operation 'gradients/add_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/add_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/add_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/add_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/add_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/add_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/add_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/add_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/add_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/add_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/MatMul_grad/MatMul' type=MatMul>,\n",
       " <tf.Operation 'gradients/MatMul_grad/MatMul_1' type=MatMul>,\n",
       " <tf.Operation 'gradients/MatMul_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/MatMul_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/MatMul_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'GradientDescent/update_Variable_1/ApplyGradientDescent' type=ApplyGradientDescent>,\n",
       " <tf.Operation 'GradientDescent/update_Variable_2/ApplyGradientDescent' type=ApplyGradientDescent>,\n",
       " <tf.Operation 'GradientDescent/update' type=NoOp>,\n",
       " <tf.Operation 'GradientDescent/value' type=Const>,\n",
       " <tf.Operation 'GradientDescent' type=AssignAdd>,\n",
       " <tf.Operation 'init' type=NoOp>]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ops.reset_default_graph()\n",
    "[op for op in tf.get_default_graph().get_operations()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "random.shuffle(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP is:  0\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  1\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  2\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  3\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  4\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  5\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  6\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  7\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  8\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  9\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  10\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  11\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  12\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  13\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  14\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  15\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  16\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  17\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  18\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "STEP is:  19\n",
      "OFFSET is:  0\n",
      "[[-0.56103279 -0.94471859 -2.98326108  1.2075727   0.60588113]\n",
      " [ 0.26127349  0.16877142 -0.71269093 -0.21745251 -0.07574096]\n",
      " [ 1.2086654  -1.38662353 -1.78111692  0.93448292 -1.17644859]\n",
      " [ 0.17953777  1.66311967  1.09515741 -0.37202257  0.54050669]\n",
      " [-1.56424974  0.07980207 -0.38657677  0.11656207  0.19618255]]\n",
      "[ 0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "from six.moves import xrange\n",
    "np.random.seed(3238)\n",
    "train_data = np.random.randn(10,5)\n",
    "train_labels = np.zeros(10)\n",
    "num_epochs = 10\n",
    "train_size = trainData.shape[0]\n",
    "BATCH_SIZE = 5\n",
    "for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n",
    "    print ('STEP is: ',step)\n",
    "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "    print ('OFFSET is: ',offset)\n",
    "    batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n",
    "    print (batch_data)\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    print (batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
