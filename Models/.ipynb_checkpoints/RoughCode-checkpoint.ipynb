{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Rough Analysis Code:\n",
    "\n",
    "\n",
    "\n",
    "# class SessionExec():\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         self.epochs = 30\n",
    "        \n",
    "#     def trainModel(self, trainDataIN, trainLabelsIN):\n",
    "#         with tf.Session() as sess:\n",
    "#             sess.run(tf.initialize_all_variables())\n",
    "\n",
    "#             for epoch in range(self.epochs): \n",
    "#                 feed_dict = {self.trainGraphDict['trainData']: trainDataIN,\n",
    "#                              self.trainGraphDict['trainLabels']: trainLabelsIN,\n",
    "#                              self.trainGraphDict['keep_prob']: 0.7\n",
    "#                         }\n",
    "\n",
    "#                 if epoch == self.epochs-1:  # Capture the weights for the last iteration.\n",
    "#                     print ('ENtering the last iteration.....', epoch)\n",
    "#                     wLRND, bLRND, _, loss, tpred = sess.run([self.trainGraphDict['weightsLRND'],\n",
    "#                                                 self.trainGraphDict['biasesLRND'],\n",
    "#                                                 self.trainGraphDict['optimizer'],\n",
    "#                                                 self.trainGraphDict['lossCE'],\n",
    "#                                                 self.trainGraphDict['trainPred']], feed_dict=feed_dict)\n",
    "#                 else:\n",
    "#                     _, loss, tpred = sess.run([self.trainGraphDict['optimizer'], \n",
    "#                                                 self.trainGraphDict['lossCE'],\n",
    "#                                                 self.trainGraphDict['trainPred']], feed_dict=feed_dict)\n",
    "\n",
    "#                 if (epoch+1)%2 == 0 or epoch==self.epochs-1:\n",
    "#                     # Evaluate training set\n",
    "#                     tacc = accuracy(tpred, trainLabelsIN)\n",
    "#                     print (\"Fold: \" + str(self.foldNUM+1) + \", Iter: \" + str(epoch+1) + \", Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(tacc))\n",
    "    \n",
    "# #             print (wLRND)\n",
    "# #             print ('')\n",
    "# #             print (bLRND)\n",
    "#             return tpred, tacc, wLRND, bLRND\n",
    "\n",
    "                \n",
    "#     # After all epoch are over with, test the model with the cross validation set    \n",
    "#     def validateModel(self, validDataIN, validLabelsIN):\n",
    "#         with tf.Session() as sess:\n",
    "#             sess.run(tf.initialize_all_variables())\n",
    "            \n",
    "#             feed_dict = {self.validGraphDict['validData']: validDataIN,\n",
    "#                      self.validGraphDict['validLabels']: validLabelsIN\n",
    "#                 }\n",
    "\n",
    "#             vpred = sess.run(self.validGraphDict['validPred'], feed_dict=feed_dict)\n",
    "            \n",
    "#             # Evaluate corss-validation set\n",
    "#             vacc = accuracy(vpred, validLabelsIN)\n",
    "#             print (\"Fold: \" + str(self.foldNUM+1) + \", Cross Validation Accuracy= \" + \"{:.5f}\".format(vacc))\n",
    "\n",
    "#             print ('')\n",
    "#             print ('')\n",
    "            \n",
    "#             return vpred, vacc \n",
    "        \n",
    "    \n",
    "    \n",
    "#     def sumWghtBias(self, cmnDictIN, newDictIN):\n",
    "#         for param, val_aray in newDictIN.items():\n",
    "#             cmnDictIN[param] = np.add(val_aray, cmnDictIN[param])    \n",
    "#         return cmnDictIN\n",
    "            \n",
    "        \n",
    "#     def execute(self, updWghtBias=False):\n",
    "#         meanValidAcc = 0\n",
    "#         self.wLRND = {}\n",
    "#         self.bLRND = {}\n",
    "#         for foldNUM, (trainDataIN, trainLabelsIN, validDataIN, validLabelsIN, labelDict) in enumerate(genTrainValidFolds(featureDIR, oneHot=True)):\n",
    "#             print ('')\n",
    "#             print ('Validation Data and Labels shape: ', validDataIN.shape, validLabelsIN.shape)\n",
    "#             print ('Training Data and Labels shape: ', trainDataIN.shape, trainLabelsIN.shape)\n",
    "#             print ('The Label Dictionary is given as: ', labelDict)\n",
    "#             self.foldNUM = foldNUM\n",
    "            \n",
    "#             reset_graph()\n",
    "            \n",
    "#             # Build a class Object\n",
    "#             objNNET = BuildNeuralNet()\n",
    "            \n",
    "#             # Build the training Graph\n",
    "#             self.trainGraphDict = objNNET.trainNet(regularization=False)\n",
    "#             tpred, tacc, wLRND, bLRND = self.trainModel(trainDataIN, trainLabelsIN)\n",
    "            \n",
    "            \n",
    "#             reset_graph()\n",
    "            \n",
    "#             # Add all the weights and biases received from each training fold\n",
    "#             if updWghtBias:\n",
    "#                 if not (self.wLRND and self.bLRND):\n",
    "#                     self.wLRND = wLRND\n",
    "#                     self.bLRND = bLRND\n",
    "#                 else:\n",
    "#                     self.wLRND = self.sumWghtBias(self.wLRND, wLRND)\n",
    "#                     self.bLRND = self.sumWghtBias(self.bLRND, bLRND)\n",
    "\n",
    "                    \n",
    "#             self.validGraphDict = objNNET.crossValid(wLRND, bLRND)\n",
    "#             vpred, vacc = self.validateModel(validDataIN, validLabelsIN)\n",
    "            \n",
    "#             trainCM = confusionMatrix(trainLabelsIN,tpred)\n",
    "#             validCM = confusionMatrix(validLabelsIN,vpred)\n",
    "\n",
    "#             meanValidAcc += vacc\n",
    "#             print ('Confusion Matrix Training Set')\n",
    "#             print (trainCM)\n",
    "#             print ('')\n",
    "#             print ('Confusion Matrix CrossValid Set')\n",
    "#             print (validCM)\n",
    "            \n",
    "# #             if foldNUM ==2:\n",
    "# #                 break\n",
    "        \n",
    "#         # Test the cross validation accuracy for the nth-fold when the weights are averaged \n",
    "#         # Find the average of all the weights and biases from the training folds and take their average\n",
    "#         if updWghtBias:\n",
    "#             wLRND = {k: v/(foldNUM+1) for k, v in self.wLRND.items()} \n",
    "#             bLRND = {k: v/(foldNUM+1) for k, v in self.bLRND.items()} \n",
    "#             self.validGraphDict = objNNET.crossValid(wLRND, bLRND)\n",
    "#             vpred, vacc = self.validateModel(validDataIN, validLabelsIN)\n",
    "#             print (\"Fold: \" + str(self.foldNUM+1) + \", Aveaged Weight Cross Validation Accuracy= \" + \"{:.5f}\".format(vacc))\n",
    "        \n",
    "#         print ('')\n",
    "#         print ('The Mean crossValidation Accuracy is: ', meanValidAcc/(foldNUM+1))\n",
    "            \n",
    "            \n",
    "# SessionExec().execute(updWghtBias=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Running i is : 0\n",
    "# # Validation Data and Labels shape:  (1000, 162) (1000, 2)\n",
    "# # Training Data and Labels shape:  (9000, 162) (9000, 2)\n",
    "# # The Label Dictionary is given as:  {0: 'trainDataAirplane.pickle', 1: 'trainDataCat.pickle'}\n",
    "# # Fold: 1, Iter: 10, Loss= 1.603019, Training Accuracy= 0.79000\n",
    "# # Fold: 1, Cross Validation Accuracy= 0.79200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "m = {}\n",
    "n = {} \n",
    "newDict = defaultdict(list)\n",
    "m['a'] = np.array([[1,2,3],[-1,3,-4]], dtype=float)\n",
    "n['a'] = np.array([[3,4,3],[-1,3,-4]], dtype=float)\n",
    "\n",
    "\n",
    "# print (np.abs(a-b))\n",
    "# print (np.sum(np.abs(a-b)))\n",
    "\n",
    "m['b'] = np.array([[5,4,1],[7,8,9]], dtype=float)\n",
    "n['b'] = np.array([[9,9,5],[-2,1,2]], dtype=float)\n",
    "\n",
    "for layer,val in n.items():\n",
    "    print (layer)\n",
    "    print (np.abs(val-m[layer]))\n",
    "    print (np.sum(np.abs(val-m[layer])))\n",
    "    print ('')\n",
    "    newDict[layer].append(np.sum(np.abs(val-m[layer])))\n",
    "    \n",
    "print (newDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "import pickle\n",
    "import random\n",
    "from skimage.feature import hog\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HOG:\n",
    "    def __init__(self, featureParams): \n",
    "        self.orienations = featureParams['orientations']\n",
    "        self.pixelsPerCell = featureParams['pixelsPerCell']\n",
    "        self.cellsPerBlock = featureParams['cellsPerBlock']\n",
    "        self.block_norm = featureParams['block_norm']\n",
    "        self.visualise = featureParams['visualise']\n",
    "        self.transform_sqrt = featureParams['transform_sqrt']\n",
    "\n",
    "    def describe(self, image):\n",
    "        # Use transform_sqrt for Power law Compression before processing the image to increase the accuracy\n",
    "        # Use visualise to return the image of the histogram\n",
    "        if self.visualise:\n",
    "            hist, hog_image = hog(image,\n",
    "                                orientations = self.orienations,\n",
    "                                pixels_per_cell = self.pixelsPerCell,\n",
    "                                cells_per_block = self.cellsPerBlock,\n",
    "                                visualise= self.visualise,\n",
    "                                transform_sqrt = self.transform_sqrt)\n",
    "            return hist, hog_image\n",
    "        else:\n",
    "            hog_image = hog(image,\n",
    "                                orientations = self.orienations,\n",
    "                                pixels_per_cell = self.pixelsPerCell,\n",
    "                                cells_per_block = self.cellsPerBlock,\n",
    "                                transform_sqrt = self.transform_sqrt)\n",
    "            return hog_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/trainDataAirplane/10009.png\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "negative dimensions are not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-4b666d669c0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m         (imgHOGp1,imgHOGp2,imgHOGp3,imgHOGp4,imgHOGp5,\n\u001b[1;32m     88\u001b[0m          \u001b[0mimgHOGp6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimgHOGp7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimgHOGp8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimgHOGp9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimgHOGp10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m          imgHOGp11,imgHOGp12,imgHOGp13,imgHOGp14) = obj_Preprocess.featureExtractor(imagePath)\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgHOGp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgHOGp2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-4b666d669c0c>\u001b[0m in \u001b[0;36mfeatureExtractor\u001b[0;34m(self, imagePath)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mimgHOGp6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_HOG_p6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgGS_crop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mimgHOGp7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_HOG_p7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgGS_crop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mimgHOGp8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_HOG_p8\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgGS_crop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mimgHOGp9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_HOG_p9\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgGS_crop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mimgHOGp10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_HOG_p10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgGS_crop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-db64af0596b2>\u001b[0m in \u001b[0;36mdescribe\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     24\u001b[0m                                 \u001b[0mpixels_per_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixelsPerCell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                 \u001b[0mcells_per_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcellsPerBlock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                                 transform_sqrt = self.transform_sqrt)\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhog_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sam/App-Setup/CondaENV/lib/python3.5/site-packages/skimage/feature/_hog.py\u001b[0m in \u001b[0;36mhog\u001b[0;34m(image, orientations, pixels_per_cell, cells_per_block, visualise, transform_sqrt, feature_vector, normalise)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mn_blocksy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_cellsy\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     normalised_blocks = np.zeros((n_blocksy, n_blocksx,\n\u001b[0;32m--> 181\u001b[0;31m                                   by, bx, orientations))\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_blocksx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: negative dimensions are not allowed"
     ]
    }
   ],
   "source": [
    "class Preprocess(): \n",
    "    \n",
    "    def __init__(self):        \n",
    "        featureParams1 = dict(orientations = 9, pixelsPerCell = (6, 6), cellsPerBlock = (1, 1), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "        featureParams2 = dict(orientations = 9, pixelsPerCell = (5, 10), cellsPerBlock = (1, 1), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "        featureParams3 = dict(orientations = 9, pixelsPerCell = (10, 5), cellsPerBlock = (1, 1), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "        \n",
    "        featureParams4 = dict(orientations = 18, pixelsPerCell = (6, 6), cellsPerBlock = (1, 1), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "        featureParams5 = dict(orientations = 18, pixelsPerCell = (5, 10), cellsPerBlock = (1, 1), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "        featureParams6 = dict(orientations = 18, pixelsPerCell = (10, 5), cellsPerBlock = (1, 1), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "        \n",
    "        \n",
    "        featureParams7 = dict(orientations = 9, pixelsPerCell = (6, 6), cellsPerBlock = (3, 3), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "        featureParams8 = dict(orientations = 9, pixelsPerCell = (5, 10), cellsPerBlock = (1, 4), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "        featureParams9 = dict(orientations = 9, pixelsPerCell = (10, 5), cellsPerBlock = (5, 2), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "        \n",
    "        featureParams10 = dict(orientations = 18, pixelsPerCell = (6, 6), cellsPerBlock = (3, 3), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "        featureParams11 = dict(orientations = 18, pixelsPerCell = (5, 10), cellsPerBlock = (2, 5), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "        featureParams12 = dict(orientations = 18, pixelsPerCell = (10, 5), cellsPerBlock = (5, 2), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "        \n",
    "        featureParams13 = dict(orientations = 18, pixelsPerCell = (5, 10), cellsPerBlock = (1, 1), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "        featureParams14 = dict(orientations = 18, pixelsPerCell = (10, 5), cellsPerBlock = (1, 1), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "        \n",
    "\n",
    "        self.obj_HOG_p1 = HOG(featureParams1)\n",
    "        self.obj_HOG_p2 = HOG(featureParams2)\n",
    "        self.obj_HOG_p3 = HOG(featureParams3)\n",
    "        self.obj_HOG_p4 = HOG(featureParams4)\n",
    "        self.obj_HOG_p5 = HOG(featureParams5)\n",
    "        self.obj_HOG_p6 = HOG(featureParams6)\n",
    "        self.obj_HOG_p7 = HOG(featureParams7)\n",
    "        self.obj_HOG_p8 = HOG(featureParams8)\n",
    "        self.obj_HOG_p9 = HOG(featureParams9)\n",
    "        self.obj_HOG_p10 = HOG(featureParams10)\n",
    "        self.obj_HOG_p11 = HOG(featureParams11)\n",
    "        self.obj_HOG_p12 = HOG(featureParams12)\n",
    "        self.obj_HOG_p13 = HOG(featureParams13)\n",
    "        self.obj_HOG_p14 = HOG(featureParams14)\n",
    "#         self.obj_HOG_p15 = HOG(featureParams15)\n",
    "#         self.obj_HOG_p3_3 = HOG(featureParams3_3)\n",
    "#         self.obj_HOG_p3_4 = HOG(featureParams3_4)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def featureExtractor (self, imagePath):\n",
    "        # Fetch the image into matrix form\n",
    "        img = cv2.imread(imagePath)\n",
    "\n",
    "        # Note tensor flow Fetches image in BGR format, hence converting it into RGB\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Convert the Image into Gray Scale\n",
    "        imgGS = cv2.cvtColor(imgRGB, cv2.COLOR_RGB2GRAY) \n",
    "        \n",
    "        imgGS_crop = imgGS[1:31, 1:31]\n",
    "\n",
    "        # Find the HOG features corresponding the parameter setting 1\n",
    "        imgHOGp1 = self.obj_HOG_p1.describe(imgGS_crop)\n",
    "        imgHOGp2 = self.obj_HOG_p2.describe(imgGS_crop)\n",
    "        imgHOGp3 = self.obj_HOG_p3.describe(imgGS_crop)\n",
    "        imgHOGp4 = self.obj_HOG_p4.describe(imgGS_crop)\n",
    "        imgHOGp5 = self.obj_HOG_p5.describe(imgGS_crop)\n",
    "        imgHOGp6 = self.obj_HOG_p6.describe(imgGS_crop)\n",
    "        imgHOGp7 = self.obj_HOG_p7.describe(imgGS_crop)\n",
    "        imgHOGp8 = self.obj_HOG_p8.describe(imgGS_crop)\n",
    "        imgHOGp9 = self.obj_HOG_p9.describe(imgGS_crop)\n",
    "        imgHOGp10 = self.obj_HOG_p10.describe(imgGS_crop)\n",
    "        imgHOGp11 = self.obj_HOG_p11.describe(imgGS_crop)\n",
    "        imgHOGp12 = self.obj_HOG_p12.describe(imgGS_crop)\n",
    "        imgHOGp13 = self.obj_HOG_p13.describe(imgGS_crop)\n",
    "        imgHOGp14 = self.obj_HOG_p14.describe(imgGS_crop)\n",
    "#         imgHOGp15 = self.obj_HOG_p11.describe(imgGS_crop)\n",
    "\n",
    "        return (imgHOGp1,imgHOGp2,imgHOGp3,imgHOGp4,imgHOGp5,imgHOGp6,imgHOGp7,imgHOGp8,imgHOGp9,imgHOGp10,imgHOGp11,imgHOGp12,imgHOGp13,imgHOGp14)\n",
    "\n",
    "airplane_datapath = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/trainDataAirplane/\"\n",
    "cat_datapath = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/trainDataCat/\"\n",
    "ImageDir = [airplane_datapath, cat_datapath]\n",
    "\n",
    "for image_dir in ImageDir:\n",
    "    objectName = os.path.basename(os.path.normpath(image_dir))\n",
    "    filenameArr =  os.listdir(image_dir)\n",
    "    for numImage, image in enumerate(filenameArr):\n",
    "        imagePath = os.path.join(image_dir, image)\n",
    "        print (imagePath)\n",
    "        obj_Preprocess = Preprocess()\n",
    "        (imgHOGp1,imgHOGp2,imgHOGp3,imgHOGp4,imgHOGp5,\n",
    "         imgHOGp6,imgHOGp7,imgHOGp8,imgHOGp9,imgHOGp10, \n",
    "         imgHOGp11,imgHOGp12,imgHOGp13,imgHOGp14) = obj_Preprocess.featureExtractor(imagePath)\n",
    "        print (imgHOGp1.shape)\n",
    "        print (imgHOGp2.shape)\n",
    "        print (imgHOGp3.shape)\n",
    "        print ('##################')\n",
    "        print (imgHOGp4.shape)\n",
    "        print (imgHOGp5.shape)\n",
    "        print (imgHOGp6.shape)\n",
    "        print ('##################')\n",
    "        print (imgHOGp7.shape)\n",
    "        print (imgHOGp8.shape)\n",
    "        print (imgHOGp9.shape)\n",
    "        print ('##################')\n",
    "        print (imgHOGp10.shape)\n",
    "        print (imgHOGp11.shape)\n",
    "        print (imgHOGp12.shape)\n",
    "        print ('##################')\n",
    "        print (imgHOGp13.shape)\n",
    "        print (imgHOGp14.shape)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
