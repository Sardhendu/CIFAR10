{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import Packages:\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import os, sys\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "mydir = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "sys.path.append(mydir)\n",
    "from DataGenerator import genTrainValidFolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STDbatch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Classification-1/STD/batchData/\"\n",
    "EDGbatch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Classification-1/EDG/batchData/\"\n",
    "HOGp1batch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Classification-1/HOGp1/batchData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def toOneHotVector():\n",
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "def reshape_data(dataset, labels, featureSize, numLabels, sample_size=None):\n",
    "    if sample_size:\n",
    "        dataset = dataset[:sample_size].reshape(sample_size, featureSize) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(numLabels) == labels[:,None]).astype(np.float32)\n",
    "    else:\n",
    "        dataset = dataset.reshape(len(dataset), featureSize) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(numLabels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BuildNeuralNet():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        seed = 2316\n",
    "        self.featureSize = 162\n",
    "        self.numHidden1 = 1000\n",
    "        self.numHidden2 = 1000\n",
    "        self.numHidden3 = 1000\n",
    "        self.numLabels = 2\n",
    "        self.alpha = 0.02\n",
    "        self.momentum = 0.9\n",
    "        \n",
    "        self.weights = {\n",
    "            \"inp_to_hid1_wght\": tf.Variable(tf.random_normal([self.featureSize, self.numHidden1], seed=seed)),\n",
    "            \"hid1_to_hid2_wght\" : tf.Variable(tf.random_normal([self.numHidden1, self.numHidden2], seed=seed)),\n",
    "            \"hid2_to_hid3_wght\": tf.Variable(tf.random_normal([self.numHidden2, self.numHidden3], seed=seed)),\n",
    "            \"hid3_to_out_wght\": tf.Variable(tf.random_normal([self.numHidden3, self.numLabels], seed=seed))\n",
    "        }\n",
    "        \n",
    "        self.biases = {\n",
    "            \"hid1_bias\": tf.Variable(tf.random_normal([self.numHidden1], seed=seed)),\n",
    "            \"hid2_bias\": tf.Variable(tf.random_normal([self.numHidden2], seed=seed)),\n",
    "            \"hid3_bias\": tf.Variable(tf.random_normal([self.numHidden3], seed=seed)),\n",
    "            \"out_bias\": tf.Variable(tf.random_normal([self.numLabels], seed=seed))\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def trainNet(self):\n",
    "        trainData = tf.placeholder(tf.float32, [None, self.featureSize])\n",
    "        trainLabels = tf.placeholder(tf.float32, [None, self.numLabels])\n",
    "\n",
    "        # 1st Hidden Layer State\n",
    "        inp_to_hid1 = tf.matmul(trainData, self.weights['inp_to_hid1_wght']) + self.biases['hid1_bias']\n",
    "        hid1State = tf.sigmoid(inp_to_hid1)\n",
    "        \n",
    "        # 2nd Hidden Layer State\n",
    "        hid1_to_hid2 = tf.matmul(hid1State, self.weights['hid1_to_hid2_wght']) + self.biases['hid2_bias']\n",
    "        hid2State = tf.sigmoid(hid1_to_hid2)\n",
    "        \n",
    "        # 3nd Hidden Layer State\n",
    "        hid2_to_hid3 = tf.matmul(hid2State, self.weights['hid2_to_hid3_wght']) + self.biases['hid3_bias']\n",
    "        hid3State = tf.sigmoid(hid2_to_hid3)\n",
    "        \n",
    "        # Output Layer State\n",
    "        hid3_to_out = tf.matmul(hid3State, self.weights['hid3_to_out_wght']) + self.biases['out_bias']\n",
    "        outState = tf.nn.softmax(hid3_to_out)\n",
    "        \n",
    "        # Loss Function and Optimization\n",
    "        lossCE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hid3_to_out, labels=trainLabels))\n",
    "#         optimizer = tf.train.AdamOptimizer(learning_rate=self.alpha).minimize(lossCE)\n",
    "        optimizer = tf.train.MomentumOptimizer(self.alpha, \n",
    "                                            self.momentum, \n",
    "                                            use_locking=False, \n",
    "                                            name='Momentum', \n",
    "                                            use_nesterov=True).minimize(lossCE)\n",
    "        \n",
    "        # Evaluate model\n",
    "        trainPred = tf.equal(tf.argmax(outState, 1), tf.argmax(trainLabels, 1))\n",
    "        trainAccuracy = tf.reduce_mean(tf.cast(trainPred, tf.float32))\n",
    "        \n",
    "        return dict(\n",
    "            trainData = trainData, \n",
    "            trainLabels = trainLabels,\n",
    "            weightsLRND = self.weights,\n",
    "            biasesLRND = self.biases,\n",
    "            optimizer = optimizer,\n",
    "            lossCE = lossCE,\n",
    "            accuracy = trainAccuracy\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def crossValid(self):\n",
    "            validData = tf.placeholder(tf.float32, [None, self.featureSize])\n",
    "            validLabels = tf.placeholder(tf.float32, [None, self.numLabels])\n",
    "\n",
    "            # 1st Hidden Layer State\n",
    "            valid_hid1State = tf.sigmoid(tf.matmul(validData, self.weights['inp_to_hid1_wght']) + self.biases['hid1_bias'])\n",
    "\n",
    "            # 2nd Hidden Layer State\n",
    "            valid_hid2State = tf.sigmoid(tf.matmul(valid_hid1State, self.weights['hid1_to_hid2_wght']) + self.biases['hid2_bias'])\n",
    "\n",
    "            # 3nd Hidden Layer State\n",
    "            valid_hid3State = tf.sigmoid(tf.matmul(valid_hid2State, self.weights['hid2_to_hid3_wght']) + self.biases['hid3_bias'])\n",
    "\n",
    "            # Output Layer State\n",
    "            validOutState = tf.nn.softmax(tf.matmul(valid_hid3State, self.weights['hid3_to_out_wght']) + self.biases['out_bias'])\n",
    "\n",
    "            validPred = tf.equal(tf.argmax(validOutState, 1), tf.argmax(validLabels, 1))\n",
    "            validAccuracy = tf.reduce_mean(tf.cast(validPred, tf.float32))\n",
    "            \n",
    "            return dict(\n",
    "                validData = validData,\n",
    "                validLabels = validLabels,\n",
    "#                 validwghts = self.weights,\n",
    "#                 validbias = self.biases,\n",
    "#                 validPred = validPred,\n",
    "                validAccuracy = validAccuracy\n",
    "            )\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trainLabels': <tf.Tensor 'Placeholder_1:0' shape=(?, 2) dtype=float32>, 'weightsLRND': {'inp_to_hid1_wght': <tensorflow.python.ops.variables.Variable object at 0x11d3b6400>, 'hid3_to_out_wght': <tensorflow.python.ops.variables.Variable object at 0x11e2c5f28>, 'hid2_to_hid3_wght': <tensorflow.python.ops.variables.Variable object at 0x11d851f60>, 'hid1_to_hid2_wght': <tensorflow.python.ops.variables.Variable object at 0x10ac82390>}, 'biasesLRND': {'hid3_bias': <tensorflow.python.ops.variables.Variable object at 0x11e2c55c0>, 'out_bias': <tensorflow.python.ops.variables.Variable object at 0x11e2c5358>, 'hid2_bias': <tensorflow.python.ops.variables.Variable object at 0x11d6dca20>, 'hid1_bias': <tensorflow.python.ops.variables.Variable object at 0x11e2c5390>}, 'accuracy': <tf.Tensor 'Mean_1:0' shape=() dtype=float32>, 'lossCE': <tf.Tensor 'Mean:0' shape=() dtype=float32>, 'trainData': <tf.Tensor 'Placeholder:0' shape=(?, 162) dtype=float32>, 'optimizer': <tensorflow.python.framework.ops.Operation object at 0x11e90acc0>}\n",
      "\n",
      "{'validAccuracy': <tf.Tensor 'Mean_2:0' shape=() dtype=float32>, 'validLabels': <tf.Tensor 'Placeholder_3:0' shape=(?, 2) dtype=float32>, 'validData': <tf.Tensor 'Placeholder_2:0' shape=(?, 162) dtype=float32>}\n",
      "Running i is : 0\n",
      "Validation Data and Labels shape:  (1000, 162) (1000, 2)\n",
      "Training Data and Labels shape:  (9000, 162) (9000, 2)\n",
      "The Label Dictionary is given as:  {0: 'trainDataAirplane.pickle', 1: 'trainDataCat.pickle'}\n",
      "Fold: 1, Iter: 0, Loss= 8.119655, Training Accuracy= 0.50700\n",
      "Fold: 1, Iter: 1, Loss= 10.969023, Training Accuracy= 0.49444\n",
      "Fold: 1, Iter: 2, Loss= 3.147324, Training Accuracy= 0.58311\n",
      "Fold: 1, Iter: 3, Loss= 2.886378, Training Accuracy= 0.60944\n",
      "Fold: 1, Iter: 4, Loss= 3.395161, Training Accuracy= 0.64133\n",
      "Fold: 1, Iter: 5, Loss= 2.871589, Training Accuracy= 0.65478\n",
      "Fold: 1, Iter: 6, Loss= 2.460382, Training Accuracy= 0.72211\n",
      "Fold: 1, Iter: 7, Loss= 1.793284, Training Accuracy= 0.75767\n",
      "Fold: 1, Iter: 8, Loss= 1.639954, Training Accuracy= 0.78022\n",
      "Fold: 1, Iter: 9, Loss= 1.603019, Training Accuracy= 0.79000\n",
      "Fold: 1, Iter: 10, Loss= 1.569964, Training Accuracy= 0.79589\n",
      "Fold: 1, Iter: 11, Loss= 1.535357, Training Accuracy= 0.80100\n",
      "Fold: 1, Iter: 12, Loss= 1.497784, Training Accuracy= 0.80400\n",
      "Fold: 1, Iter: 13, Loss= 1.457155, Training Accuracy= 0.80811\n",
      "Fold: 1, Iter: 14, Loss= 1.414268, Training Accuracy= 0.81111\n",
      "Fold: 1, Iter: 15, Loss= 1.369966, Training Accuracy= 0.81633\n",
      "Fold: 1, Iter: 16, Loss= 1.324851, Training Accuracy= 0.81778\n",
      "Fold: 1, Iter: 17, Loss= 1.279587, Training Accuracy= 0.81833\n",
      "Fold: 1, Iter: 18, Loss= 1.234840, Training Accuracy= 0.82167\n",
      "Fold: 1, Iter: 19, Loss= 1.191227, Training Accuracy= 0.82289\n",
      "Fold: 1, Iter: 20, Loss= 1.149524, Training Accuracy= 0.82522\n",
      "Fold: 1, Iter: 21, Loss= 1.110455, Training Accuracy= 0.82444\n",
      "Fold: 1, Iter: 22, Loss= 1.074516, Training Accuracy= 0.82456\n",
      "Fold: 1, Iter: 23, Loss= 1.042036, Training Accuracy= 0.82456\n",
      "Fold: 1, Iter: 24, Loss= 1.013256, Training Accuracy= 0.82522\n",
      "Fold: 1, Iter: 25, Loss= 0.988228, Training Accuracy= 0.82456\n",
      "Fold: 1, Iter: 26, Loss= 0.966572, Training Accuracy= 0.82144\n",
      "Fold: 1, Iter: 27, Loss= 0.947561, Training Accuracy= 0.82244\n",
      "Fold: 1, Iter: 28, Loss= 0.930419, Training Accuracy= 0.82189\n",
      "Fold: 1, Iter: 29, Loss= 0.914501, Training Accuracy= 0.82100\n",
      "\n",
      "Fold: 1, Cross Validation Accuracy= 0.79700\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "def model(trainGraphDict, validGraphDict):\n",
    "    print (trainGraphDict)\n",
    "    print ('')\n",
    "    print (validGraphDict)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for foldNUM, (trainDataIN, trainLabelsIN, validDataIN, validLabelsIN, labelDict) in enumerate(genTrainValidFolds(HOGp1batch_dir, oneHot=True)):\n",
    "            print ('Validation Data and Labels shape: ', validDataIN.shape, validLabelsIN.shape)\n",
    "            print ('Training Data and Labels shape: ', trainDataIN.shape, trainLabelsIN.shape)\n",
    "            print ('The Label Dictionary is given as: ', labelDict)\n",
    "\n",
    "            for epoch in range(epochs): \n",
    "                feed_dict = {trainGraphDict['trainData']: trainDataIN,\n",
    "                             trainGraphDict['trainLabels']: trainLabelsIN\n",
    "                        }\n",
    "                \n",
    "                \n",
    "                _, loss, tacc, twghts, tbiases = sess.run([trainGraphDict['optimizer'], \n",
    "                                                        trainGraphDict['lossCE'], \n",
    "                                                        trainGraphDict['accuracy'],\n",
    "                                                        trainGraphDict['weightsLRND'],\n",
    "                                                        trainGraphDict['biasesLRND']], feed_dict=feed_dict)\n",
    "\n",
    "                print (\"Fold: \" + str(foldNUM+1) + \", Iter: \" + str(epoch) + \", Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(tacc))\n",
    "\n",
    "                # During the Last iteration of the nth fold batch, we should calculate the model accuracy with Cross Validataion Data\n",
    "                if epoch==epochs-1:\n",
    "                    print ('')\n",
    "\n",
    "                    feed_dict = {validGraphDict['validData']: validDataIN,\n",
    "                             validGraphDict['validLabels']: validLabelsIN\n",
    "                        }\n",
    "                \n",
    "                    vacc = sess.run(validGraphDict['validAccuracy'], feed_dict=feed_dict)\n",
    "                    print (\"Fold: \" + str(foldNUM+1) + \", Cross Validation Accuracy= \" + \"{:.5f}\".format(vacc))\n",
    "                    \n",
    "            print ('')\n",
    "            print ('')\n",
    "\n",
    "            break\n",
    "#             epoch += 1\n",
    "#         print (\"Optimization Finished!\")\n",
    "#     obj_SVM = Models()\n",
    "#     batchEvalDict[foldNUM] = obj_SVM.classify(trainData, trainLabels, validData)\n",
    "\n",
    "reset_graph()\n",
    "objNNET = BuildNeuralNet()\n",
    "trainGraphDict = objNNET.trainNet()\n",
    "validGraphDict = objNNET.crossValid()\n",
    "model(trainGraphDict, validGraphDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
