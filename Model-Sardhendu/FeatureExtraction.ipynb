{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "import pickle\n",
    "from skimage.feature import hog\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "airplane_datapath = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/trainDataAirplane/\"\n",
    "cat_datapath = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/trainDataCat/\"\n",
    "ImageDir = [airplane_datapath, cat_datapath]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Below Process successfully creates the data set for the features:\n",
    "\n",
    "1. Standarized (Normalized) Image\n",
    "2. Image with Edge activation pixel\n",
    "3. Image with HOG Features (One Filter Type)\n",
    "4. To do ..... (HOG with multple filters)\n",
    "    \n",
    "The below code is generalized to store the features as ndarrays where,\n",
    "\n",
    "--> The number of rows are the number of images,\n",
    "--> The number of columns are the flattened feature set.\n",
    "\n",
    "The dataset is compressed into pickle files and stored in their respective directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def featureStandarize(image_pxlvals):\n",
    "    return(image_pxlvals - 255.0/2)/255.0\n",
    "\n",
    "\n",
    "# We choose 18 orientation for the object recognition task\n",
    "class HOG:\n",
    "    def __init__(self, featureParams): \n",
    "        self.orienations = featureParams['orientations']\n",
    "        self.pixelsPerCell = featureParams['pixelsPerCell']\n",
    "        self.cellsPerBlock = featureParams['cellsPerBlock']\n",
    "        self.block_norm = featureParams['block_norm']\n",
    "        self.visualise = featureParams['visualise']\n",
    "        self.transform_sqrt = featureParams['transform_sqrt']\n",
    "\n",
    "    def describe(self, image):\n",
    "        # Use transform_sqrt for Power law Compression before processing the image to increase the accuracy\n",
    "        # Use visualise to return the image of the histogram\n",
    "        if self.visualise:\n",
    "            hist, hog_image = hog(image,\n",
    "                                orientations = self.orienations,\n",
    "                                pixels_per_cell = self.pixelsPerCell,\n",
    "                                cells_per_block = self.cellsPerBlock,\n",
    "                                visualise= self.visualise,\n",
    "                                transform_sqrt = self.transform_sqrt)\n",
    "            return hist, hog_image\n",
    "        else:\n",
    "            hog_image = hog(image,\n",
    "                                orientations = self.orienations,\n",
    "                                pixels_per_cell = self.pixelsPerCell,\n",
    "                                cells_per_block = self.cellsPerBlock,\n",
    "                                transform_sqrt = self.transform_sqrt)\n",
    "            return hog_image\n",
    "\n",
    "\n",
    "def featureExtraction(pathTo_images, filenameArr, imageSize=32, mimNumImage=None, numChannels=3):\n",
    "    datasetSTD = np.ndarray(shape=(len(filenameArr), imageSize, imageSize), dtype=np.float32)\n",
    "    datasetEDG = np.ndarray(shape=(len(filenameArr), imageSize, imageSize), dtype=np.float32)\n",
    "    datasetHOGp1 = []\n",
    "    datasetHOGp2 = []\n",
    "    datasetHOGp3 = []\n",
    "#     datasetEDG = np.ndarray(shape=(len(filenameArr), imageSize, imageSize), dtype=np.float32)\n",
    "    \n",
    "    featureParams1 = dict(orientations = 18, pixelsPerCell = (9, 9), cellsPerBlock = (1, 1), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "    featureParams2 = dict(orientations = 18, pixelsPerCell = (5, 9), cellsPerBlock = (1, 1), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "    \n",
    "    featureParams3_1 = dict(orientations = 18, pixelsPerCell = (5, 9), cellsPerBlock = (1, 1), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "    featureParams3_2 = dict(orientations = 18, pixelsPerCell = (6, 6), cellsPerBlock = (1, 1), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "    featureParams3_3 = dict(orientations = 18, pixelsPerCell = (9, 5), cellsPerBlock = (1, 1), block_norm = 'L1', visualise = False, transform_sqrt = True)\n",
    "    \n",
    "    obj_HOG_p1 = HOG(featureParams1)\n",
    "    obj_HOG_p2 = HOG(featureParams2)\n",
    "    obj_HOG_p3_1 = HOG(featureParams3_1)\n",
    "    obj_HOG_p3_2 = HOG(featureParams3_2)\n",
    "    obj_HOG_p3_3 = HOG(featureParams3_3)\n",
    "    \n",
    "    for numImage, image in enumerate(filenameArr):\n",
    "        imagePath = os.path.join(pathTo_images, image)\n",
    "#         print (numImage)\n",
    "        try:\n",
    "            # Get the Image\n",
    "#             print ('The input image path is ', imagePath)\n",
    "            img = cv2.imread(imagePath)\n",
    "            \n",
    "            # Convert the Image into Gray Scale\n",
    "            imgGS = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "\n",
    "            # Get normalized image\n",
    "            imgSTD = featureStandarize(imgGS)\n",
    "\n",
    "            # Blurr the Gray Scale Image using a Gaussian Blurr\n",
    "            imgBLR = cv2.GaussianBlur(imgGS, (3,3), 0)                 # The filer size is chosen to be 3 and the standard deviation for the distribution is 0\n",
    "                      \n",
    "            # Detect Edges using Canny Filter\n",
    "            imgEDG = cv2.Canny(imgBLR, 30, 150)                        # The minimum threshold value chosen is 60 and the maximum threshold chosen is 150\n",
    "            \n",
    "            # Find the HOG features corresponding the parameter setting 1\n",
    "            imgHOGp1 = obj_HOG_p1.describe(imgGS)                      # We collect the HOG image pertaining to the first parameter settings\n",
    "            \n",
    "            # Find the HOG features corresponding the parameter setting 2\n",
    "            imgHOGp2 = obj_HOG_p2.describe(imgGS)                      # We collect the HOG image pertaining to the first parameter settings\n",
    "            \n",
    "            # MultiHOG kernels stack together, the HOG features corresponding the parameter setting 3\n",
    "            imgHOGp3_1 = obj_HOG_p3_1.describe(imgGS)                      # We collect the HOG image pertaining to the first parameter settings\n",
    "            imgHOGp3_2 = obj_HOG_p3_2.describe(imgGS)\n",
    "            imgHOGp3_3 = obj_HOG_p3_3.describe(imgGS)\n",
    "                            \n",
    "#             print (imgHOGp3_1.shape)\n",
    "#             print (imgHOGp3_2.shape)\n",
    "#             print (imgHOGp3_3.shape)\n",
    "            \n",
    "            datasetSTD[numImage, :] = imgSTD\n",
    "            datasetEDG[numImage, :] = imgEDG\n",
    "            datasetHOGp1.append(imgHOGp1)\n",
    "            datasetHOGp2.append(imgHOGp2)\n",
    "            datasetHOGp3.append(np.hstack((imgHOGp3_1,imgHOGp3_2,imgHOGp3_3)))\n",
    "            \n",
    "            \n",
    "#             if numImage%1000 == 0:\n",
    "#                 print ('For image number: ', numImage)\n",
    "#                 print ('Count of Non Zero pixels entries in img are: ', len(np.where(np.reshape(img, 32*32*3)!=0)[0]))\n",
    "#                 print ('Count of Non Zero pixels entries in imgGS are: ', len(np.where(np.reshape(imgGS, 32*32)!=0)[0]))\n",
    "#                 print ('Count of Non Zero pixels entries in imgBLR are: ', len(np.where(np.reshape(imgBLR, 32*32)!=0)[0]))  \n",
    "#                 print ('Count of Non Zero pixels entries in imgEDG are: ', len(np.where(np.reshape(imgEDG, 32*32)!=0)[0])) \n",
    "#                 print ('Count of Non Zero pixels entries in imgHOGp1 are: ', len(np.where(imgHOGp1!=0)[0]))\n",
    "#                 print ('Count of Non Zero pixels entries in imgHOGp2 are: ', len(np.where(imgHOGp2!=0)[0]))\n",
    "#                 print ('Count of Non Zero pixels entries in imgHOGp3_1 are: ', len(np.where(imgHOGp3_1!=0)[0]))\n",
    "#                 print ('Count of Non Zero pixels entries in imgHOGp3_2 are: ', len(np.where(imgHOGp3_2!=0)[0]))\n",
    "#                 print ('Count of Non Zero pixels entries in imgHOGp3_2 are: ', len(np.where(imgHOGp3_3!=0)[0]))                \n",
    "#                 print ('')\n",
    "                \n",
    "        \n",
    "        except IOError as e:\n",
    "            print('Could not read:', image, ':', e, '- hence skipping.')\n",
    "    \n",
    "    return (datasetSTD.reshape((-1,imageSize*imageSize)), \n",
    "            datasetEDG.reshape((-1,imageSize*imageSize)), \n",
    "            np.array(datasetHOGp1), \n",
    "            np.array(datasetHOGp2), \n",
    "            np.array(datasetHOGp3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The current image directory is:  /Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/trainDataAirplane/\n",
      "Standarized Feature DataSet: shape =  (5000, 1024)\n",
      "Edge Feature DataSet: shape =  (5000, 1024)\n",
      "HOG param1 Feature DataSet: shape =  (5000, 162)\n",
      "HOG param2 Feature DataSet: shape =  (5000, 324)\n",
      "HOG param3 Feature DataSet: shape =  (5000, 1098)\n",
      "Storing data for STD Feature set\n",
      "Storing data for EDGE Feature set\n",
      "Storing data for HOG p1 Feature set\n",
      "Storing data for HOG p2 Feature set\n",
      "Storing data for HOG p3 Feature set\n",
      "\n",
      "The current image directory is:  /Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/trainDataCat/\n",
      "Standarized Feature DataSet: shape =  (5000, 1024)\n",
      "Edge Feature DataSet: shape =  (5000, 1024)\n",
      "HOG param1 Feature DataSet: shape =  (5000, 162)\n",
      "HOG param2 Feature DataSet: shape =  (5000, 324)\n",
      "HOG param3 Feature DataSet: shape =  (5000, 1098)\n",
      "Storing data for STD Feature set\n",
      "Storing data for EDGE Feature set\n",
      "Storing data for HOG p1 Feature set\n",
      "Storing data for HOG p2 Feature set\n",
      "Storing data for HOG p3 Feature set\n"
     ]
    }
   ],
   "source": [
    "featureSTDPath = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/STD/\"\n",
    "featureEDGPath = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/EDG/\"\n",
    "featureHOGp1Path = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/HOGp1/\"\n",
    "featureHOGp2Path = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/HOGp2/\"\n",
    "featureHOGp3Path = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/HOGp3/\"\n",
    "\n",
    "DataDir = [featureSTDPath, featureEDGPath, featureHOGp1Path, featureHOGp2Path, featureHOGp3Path]\n",
    "\n",
    "def main(forceDump=None, reduceDimension=500):\n",
    "    for image_dir in ImageDir:\n",
    "        objectName = os.path.basename(os.path.normpath(image_dir))\n",
    "        filenameArr =  os.listdir(image_dir)\n",
    "        print ('')\n",
    "        print ('The current image directory is: ', image_dir)\n",
    "        (datasetSTD, datasetEDG, datasetHOGp1, datasetHOGp2, datasetHOGp3) = featureExtraction(image_dir, filenameArr)\n",
    "        \n",
    "        if reduceDimension:\n",
    "            decompose = PCA(n_components=reduceDimension)\n",
    "            datasetHOGp3 = decompose.fit_transform(datasetHOGp3)\n",
    "#             print(decompose.explained_variance_ratio_) \n",
    "        \n",
    "        print ('Standarized Feature DataSet: shape = ', datasetSTD.shape)\n",
    "        print ('Edge Feature DataSet: shape = ', datasetEDG.shape)\n",
    "        print ('HOG param1 Feature DataSet: shape = ', datasetHOGp1.shape)\n",
    "        print ('HOG param2 Feature DataSet: shape = ', datasetHOGp2.shape)\n",
    "        print ('HOG param3 Feature DataSet: shape = ', datasetHOGp3.shape)\n",
    "        \n",
    "\n",
    "        for data_dir in DataDir:        \n",
    "            if not os.path.exists(data_dir):\n",
    "                os.makedirs(data_dir)\n",
    "                \n",
    "            featureType = os.path.basename(os.path.normpath(data_dir))    \n",
    "            fileName = data_dir+objectName+\".pickle\"\n",
    "\n",
    "            # DUMP PICKLE FILES\n",
    "            if os.path.exists(fileName) and not forceDump:\n",
    "                print ('The path already exists, you should force the dump')\n",
    "            else:\n",
    "                try:\n",
    "                    with open(fileName, 'wb') as f:\n",
    "                        if featureType=='STD':\n",
    "                            print ('Storing data for STD Feature set')\n",
    "                            pickle.dump(datasetSTD, f, pickle.HIGHEST_PROTOCOL)\n",
    "                        elif featureType=='EDG':\n",
    "                            print ('Storing data for EDGE Feature set')\n",
    "                            pickle.dump(datasetEDG, f, pickle.HIGHEST_PROTOCOL)\n",
    "                        elif featureType=='HOGp1':\n",
    "                            print ('Storing data for HOG p1 Feature set')\n",
    "                            pickle.dump(datasetHOGp1, f, pickle.HIGHEST_PROTOCOL)\n",
    "                        elif featureType=='HOGp2':\n",
    "                            print ('Storing data for HOG p2 Feature set')\n",
    "                            pickle.dump(datasetHOGp2, f, pickle.HIGHEST_PROTOCOL)\n",
    "                        elif featureType=='HOGp3':\n",
    "                            print ('Storing data for HOG p3 Feature set')\n",
    "                            pickle.dump(datasetHOGp3, f, pickle.HIGHEST_PROTOCOL)\n",
    "                except Exception as e:\n",
    "                    print('Unable to save data to', fileName1, ':', e)\n",
    "                    \n",
    "    \n",
    "main(forceDump=1, reduceDimension=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> The Below code calls the class CreateBatches. Class CreateBatches \n",
    "\n",
    "1. Gets the pickled feature data from the directory, \n",
    "2. ranandomize the data\n",
    "3. Divides the into train and test dataset. (In this case we dont code for test data as we have the test data as a different test file)\n",
    "4. Finally, the training data is converted into 10 folds and stored into the respective directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys,os\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "mydir = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "sys.path.append(mydir)\n",
    "\n",
    "from DataPreparation import CreateBatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed use for randomness is :  8653\n",
      "The training Data set size is :  (10000, 1024)\n",
      "The training Labels size is :  (10000,)\n",
      "The test Data set size is :  (0, 1024)\n",
      "The test Labels size is :  (0,)\n",
      "Batch No:  0  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  0  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  1  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  1  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  2  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  2  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  3  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  3  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  4  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  4  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  5  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  5  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  6  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  6  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  7  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  7  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  8  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  8  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  9  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  9  : Training Batch Labels Shape : (1000,)\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# Standarized Feature Set\n",
    "#########################\n",
    "\n",
    "# Create 10 Batches and stores in into the provided Batch Directory for the Strandarized Feature set of Images\n",
    "imageDim = 32*32\n",
    "numBatches =10\n",
    "maxNumImage = 5000\n",
    "test_percntg = 0\n",
    "\n",
    "root_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/STD/\"\n",
    "batch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/STD/batchData/\"\n",
    "\n",
    "obj_STD = CreateBatches(dimensions=imageDim)\n",
    "trainData, trainLabels, _, _, labelDict = obj_STD.gen_TrainTestData(max_num_images=maxNumImage, dir_to_pickle_files=root_dir, test_percntg=test_percntg)\n",
    "\n",
    "# print (labelDict)\n",
    "for batchNum, (trnBatchData, trnBatchLabel) in enumerate(obj_STD.generateBatches(dataset=trainData, labels=trainLabels, numBatches=numBatches)):\n",
    "    obj_STD.dumpBatches(batch_dir, trnBatchData, trnBatchLabel, batchNum=batchNum, labelDict=labelDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed use for randomness is :  8653\n",
      "The training Data set size is :  (10000, 1024)\n",
      "The training Labels size is :  (10000,)\n",
      "The test Data set size is :  (0, 1024)\n",
      "The test Labels size is :  (0,)\n",
      "Batch No:  0  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  0  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  1  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  1  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  2  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  2  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  3  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  3  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  4  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  4  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  5  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  5  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  6  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  6  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  7  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  7  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  8  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  8  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  9  : Training Batch Data Shape: (1000, 1024)\n",
      "Batch No:  9  : Training Batch Labels Shape : (1000,)\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# Edge Feature Set\n",
    "#########################\n",
    "\n",
    "# Create 10 Batches and stores in into the provided Batch Directory for the Edge Feature set of Images\n",
    "imageDim=32*32\n",
    "numBatches =10\n",
    "maxNumImage = 5000\n",
    "test_percntg=0\n",
    "\n",
    "root_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/EDG/\"\n",
    "batch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/EDG/batchData/\"\n",
    "\n",
    "obj_EDG = CreateBatches(dimensions=imageDim)\n",
    "trainData, trainLabels, _, _, labelDict = obj_EDG.gen_TrainTestData(max_num_images=maxNumImage, dir_to_pickle_files=root_dir, test_percntg=test_percntg)\n",
    "\n",
    "for batchNum, (trnBatchData, trnBatchLabel) in enumerate(obj_EDG.generateBatches(dataset=trainData, labels=trainLabels, numBatches=numBatches)):\n",
    "    obj_EDG.dumpBatches(batch_dir, trnBatchData, trnBatchLabel, batchNum=batchNum, labelDict=labelDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed use for randomness is :  8653\n",
      "The training Data set size is :  (10000, 72)\n",
      "The training Labels size is :  (10000,)\n",
      "The test Data set size is :  (0, 72)\n",
      "The test Labels size is :  (0,)\n",
      "Batch No:  0  : Training Batch Data Shape: (1000, 72)\n",
      "Batch No:  0  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  1  : Training Batch Data Shape: (1000, 72)\n",
      "Batch No:  1  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  2  : Training Batch Data Shape: (1000, 72)\n",
      "Batch No:  2  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  3  : Training Batch Data Shape: (1000, 72)\n",
      "Batch No:  3  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  4  : Training Batch Data Shape: (1000, 72)\n",
      "Batch No:  4  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  5  : Training Batch Data Shape: (1000, 72)\n",
      "Batch No:  5  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  6  : Training Batch Data Shape: (1000, 72)\n",
      "Batch No:  6  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  7  : Training Batch Data Shape: (1000, 72)\n",
      "Batch No:  7  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  8  : Training Batch Data Shape: (1000, 72)\n",
      "Batch No:  8  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  9  : Training Batch Data Shape: (1000, 72)\n",
      "Batch No:  9  : Training Batch Labels Shape : (1000,)\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# HOG p1 Feature Set\n",
    "#########################\n",
    "\n",
    "# Create 10 Batches and stores in into the provided Batch Directory for the Hog Feature with first parameter settings for the set of Images\n",
    "imageDim=72\n",
    "numBatches =10\n",
    "maxNumImage = 5000\n",
    "test_percntg=0\n",
    "\n",
    "root_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/HOGp1/\"\n",
    "batch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/HOGp1/batchData/\"\n",
    "\n",
    "obj_HOGp1 = CreateBatches(dimensions=imageDim)\n",
    "trainData, trainLabels, _, _, labelDict = obj_HOGp1.gen_TrainTestData(max_num_images=maxNumImage, dir_to_pickle_files=root_dir, test_percntg=test_percntg)\n",
    "\n",
    "for batchNum, (trnBatchData, trnBatchLabel) in enumerate(obj_HOGp1.generateBatches(dataset=trainData, labels=trainLabels, numBatches=numBatches)):\n",
    "    obj_HOGp1.dumpBatches(batch_dir, trnBatchData, trnBatchLabel, batchNum=batchNum, labelDict=labelDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed use for randomness is :  8653\n",
      "The training Data set size is :  (10000, 324)\n",
      "The training Labels size is :  (10000,)\n",
      "The test Data set size is :  (0, 324)\n",
      "The test Labels size is :  (0,)\n",
      "Batch No:  0  : Training Batch Data Shape: (1000, 324)\n",
      "Batch No:  0  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  1  : Training Batch Data Shape: (1000, 324)\n",
      "Batch No:  1  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  2  : Training Batch Data Shape: (1000, 324)\n",
      "Batch No:  2  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  3  : Training Batch Data Shape: (1000, 324)\n",
      "Batch No:  3  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  4  : Training Batch Data Shape: (1000, 324)\n",
      "Batch No:  4  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  5  : Training Batch Data Shape: (1000, 324)\n",
      "Batch No:  5  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  6  : Training Batch Data Shape: (1000, 324)\n",
      "Batch No:  6  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  7  : Training Batch Data Shape: (1000, 324)\n",
      "Batch No:  7  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  8  : Training Batch Data Shape: (1000, 324)\n",
      "Batch No:  8  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  9  : Training Batch Data Shape: (1000, 324)\n",
      "Batch No:  9  : Training Batch Labels Shape : (1000,)\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# HOG p2 Feature Set\n",
    "#########################\n",
    "\n",
    "# Create 10 Batches and stores in into the provided Batch Directory for the Hog Feature with first parameter settings for the set of Images\n",
    "imageDim=324\n",
    "numBatches =10\n",
    "maxNumImage = 5000\n",
    "test_percntg=0\n",
    "\n",
    "root_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/HOGp2/\"\n",
    "batch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/HOGp2/batchData/\"\n",
    "\n",
    "obj_HOGp2 = CreateBatches(dimensions=imageDim)\n",
    "trainData, trainLabels, _, _, labelDict = obj_HOGp2.gen_TrainTestData(max_num_images=maxNumImage, dir_to_pickle_files=root_dir, test_percntg=test_percntg)\n",
    "\n",
    "for batchNum, (trnBatchData, trnBatchLabel) in enumerate(obj_HOGp2.generateBatches(dataset=trainData, labels=trainLabels, numBatches=numBatches)):\n",
    "    obj_HOGp2.dumpBatches(batch_dir, trnBatchData, trnBatchLabel, batchNum=batchNum, labelDict=labelDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed use for randomness is :  8653\n",
      "The training Data set size is :  (10000, 1098)\n",
      "The training Labels size is :  (10000,)\n",
      "The test Data set size is :  (0, 1098)\n",
      "The test Labels size is :  (0,)\n",
      "Batch No:  0  : Training Batch Data Shape: (1000, 1098)\n",
      "Batch No:  0  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  1  : Training Batch Data Shape: (1000, 1098)\n",
      "Batch No:  1  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  2  : Training Batch Data Shape: (1000, 1098)\n",
      "Batch No:  2  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  3  : Training Batch Data Shape: (1000, 1098)\n",
      "Batch No:  3  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  4  : Training Batch Data Shape: (1000, 1098)\n",
      "Batch No:  4  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  5  : Training Batch Data Shape: (1000, 1098)\n",
      "Batch No:  5  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  6  : Training Batch Data Shape: (1000, 1098)\n",
      "Batch No:  6  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  7  : Training Batch Data Shape: (1000, 1098)\n",
      "Batch No:  7  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  8  : Training Batch Data Shape: (1000, 1098)\n",
      "Batch No:  8  : Training Batch Labels Shape : (1000,)\n",
      "Batch No:  9  : Training Batch Data Shape: (1000, 1098)\n",
      "Batch No:  9  : Training Batch Labels Shape : (1000,)\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# HOG p3 Feature Set\n",
    "#########################\n",
    "\n",
    "# Create 10 Batches and stores in into the provided Batch Directory for the Hog Feature with first parameter settings for the set of Images\n",
    "imageDim=1098\n",
    "numBatches =10\n",
    "maxNumImage = 5000\n",
    "test_percntg=0\n",
    "\n",
    "root_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/HOGp3/\"\n",
    "batch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/HOGp3/batchData/\"\n",
    "\n",
    "obj_HOGp3 = CreateBatches(dimensions=imageDim)\n",
    "trainData, trainLabels, _, _, labelDict = obj_HOGp3.gen_TrainTestData(max_num_images=maxNumImage, dir_to_pickle_files=root_dir, test_percntg=test_percntg)\n",
    "\n",
    "for batchNum, (trnBatchData, trnBatchLabel) in enumerate(obj_HOGp3.generateBatches(dataset=trainData, labels=trainLabels, numBatches=numBatches)):\n",
    "    obj_HOGp3.dumpBatches(batch_dir, trnBatchData, trnBatchLabel, batchNum=batchNum, labelDict=labelDict)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
