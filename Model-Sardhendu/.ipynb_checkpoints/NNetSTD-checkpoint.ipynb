{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Packages:\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "# Model Packeges import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "mydir = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "sys.path.append(mydir)\n",
    "from DataGenerator import genTrainValidFolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STDbatch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/STD/batchData/\"\n",
    "EDGbatch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/EDG/batchData/\"\n",
    "HOGp1batch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/HOGp1/batchData/\"\n",
    "HOGp2batch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/HOGp2/batchData/\"\n",
    "\n",
    "featureDIR = HOGp2batch_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def toOneHotVector():\n",
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "def reshape_data(dataset, labels, featureSize, numLabels, sample_size=None):\n",
    "    if sample_size:\n",
    "        dataset = dataset[:sample_size].reshape(sample_size, featureSize) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(numLabels) == labels[:,None]).astype(np.float32)\n",
    "    else:\n",
    "        dataset = dataset.reshape(len(dataset), featureSize) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(numLabels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BuildNeuralNet():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        seed = 2316\n",
    "        self.featureSize = 576\n",
    "        self.numHidden1 = 1000\n",
    "        self.numHidden2 = 1000\n",
    "        self.numHidden3 = 1000\n",
    "        self.numLabels = 2\n",
    "        self.alpha = 0.7\n",
    "        self.momentum = 0.9\n",
    "        \n",
    "        self.weights = {\n",
    "            \"inp_to_hid1_wght\": tf.Variable(tf.random_normal([self.featureSize, self.numHidden1], seed=seed)),\n",
    "            \"hid1_to_hid2_wght\" : tf.Variable(tf.random_normal([self.numHidden1, self.numHidden2], seed=seed)),\n",
    "            \"hid2_to_hid3_wght\": tf.Variable(tf.random_normal([self.numHidden2, self.numHidden3], seed=seed)),\n",
    "            \"hid3_to_out_wght\": tf.Variable(tf.random_normal([self.numHidden3, self.numLabels], seed=seed))\n",
    "        }\n",
    "        \n",
    "        self.biases = {\n",
    "            \"hid1_bias\": tf.Variable(tf.random_normal([self.numHidden1], seed=seed)),\n",
    "            \"hid2_bias\": tf.Variable(tf.random_normal([self.numHidden2], seed=seed)),\n",
    "            \"hid3_bias\": tf.Variable(tf.random_normal([self.numHidden3], seed=seed)),\n",
    "            \"out_bias\": tf.Variable(tf.random_normal([self.numLabels], seed=seed))\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def trainNet(self):\n",
    "        trainData = tf.placeholder(tf.float32, [None, self.featureSize])\n",
    "        trainLabels = tf.placeholder(tf.float32, [None, self.numLabels])\n",
    "\n",
    "        # 1st Hidden Layer State\n",
    "        inp_to_hid1 = tf.matmul(trainData, self.weights['inp_to_hid1_wght']) + self.biases['hid1_bias']\n",
    "        hid1State = tf.sigmoid(inp_to_hid1)\n",
    "        \n",
    "        # 2nd Hidden Layer State\n",
    "        hid1_to_hid2 = tf.matmul(hid1State, self.weights['hid1_to_hid2_wght']) + self.biases['hid2_bias']\n",
    "        hid2State = tf.sigmoid(hid1_to_hid2)\n",
    "        \n",
    "        # 3nd Hidden Layer State\n",
    "        hid2_to_hid3 = tf.matmul(hid2State, self.weights['hid2_to_hid3_wght']) + self.biases['hid3_bias']\n",
    "        hid3State = tf.sigmoid(hid2_to_hid3)\n",
    "        \n",
    "        # Output Layer State\n",
    "        hid3_to_out = tf.matmul(hid3State, self.weights['hid3_to_out_wght']) + self.biases['out_bias']\n",
    "        outState = tf.nn.softmax(hid3_to_out)\n",
    "        \n",
    "        # Loss Function and Optimization\n",
    "        lossCE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hid3_to_out, labels=trainLabels))\n",
    "#         optimizer = tf.train.AdamOptimizer(learning_rate=self.alpha).minimize(lossCE)\n",
    "        optimizer = tf.train.MomentumOptimizer(self.alpha, \n",
    "                                            self.momentum, \n",
    "                                            use_locking=False, \n",
    "                                            name='Momentum', \n",
    "                                            use_nesterov=True).minimize(lossCE)\n",
    "        \n",
    "        # Evaluate model\n",
    "#         trainPred = tf.equal(tf.argmax(outState, 1), tf.argmax(trainLabels, 1))\n",
    "#         trainAccuracy = tf.reduce_mean(tf.cast(trainPred, tf.float32))\n",
    "        \n",
    "        return dict(\n",
    "            trainData = trainData, \n",
    "            trainLabels = trainLabels,\n",
    "#             weightsLRND = self.weights,\n",
    "            trainPred = outState,\n",
    "            optimizer = optimizer,\n",
    "            lossCE = lossCE\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def crossValid(self):\n",
    "            validData = tf.placeholder(tf.float32, [None, self.featureSize])\n",
    "            validLabels = tf.placeholder(tf.float32, [None, self.numLabels])\n",
    "\n",
    "            # 1st Hidden Layer State\n",
    "            valid_hid1State = tf.sigmoid(tf.matmul(validData, self.weights['inp_to_hid1_wght']) + self.biases['hid1_bias'])\n",
    "\n",
    "            # 2nd Hidden Layer State\n",
    "            valid_hid2State = tf.sigmoid(tf.matmul(valid_hid1State, self.weights['hid1_to_hid2_wght']) + self.biases['hid2_bias'])\n",
    "\n",
    "            # 3nd Hidden Layer State\n",
    "            valid_hid3State = tf.sigmoid(tf.matmul(valid_hid2State, self.weights['hid2_to_hid3_wght']) + self.biases['hid3_bias'])\n",
    "\n",
    "            # Output Layer State\n",
    "            validOutState = tf.nn.softmax(tf.matmul(valid_hid3State, self.weights['hid3_to_out_wght']) + self.biases['out_bias'])\n",
    "\n",
    "#             validPred = tf.equal(tf.argmax(validOutState, 1), tf.argmax(validLabels, 1))\n",
    "#             validAccuracy = tf.reduce_mean(tf.cast(validPred, tf.float32))\n",
    "            \n",
    "            return dict(\n",
    "                validData = validData,\n",
    "                validLabels = validLabels,\n",
    "                validPred = validOutState\n",
    "            )\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Data and Labels shape:  (1000, 576) (1000, 2)\n",
      "Training Data and Labels shape:  (9000, 576) (9000, 2)\n",
      "The Label Dictionary is given as:  {0: 'trainDataAirplane.pickle', 1: 'trainDataCat.pickle'}\n",
      "Fold: 1, Iter: 2, Loss= 94.662552, Training Accuracy= 50.00000\n",
      "Fold: 1, Iter: 4, Loss= 107.457809, Training Accuracy= 50.00000\n",
      "Fold: 1, Iter: 6, Loss= 29.398857, Training Accuracy= 50.00000\n",
      "Fold: 1, Iter: 8, Loss= 52.210743, Training Accuracy= 50.00000\n",
      "Fold: 1, Iter: 10, Loss= 8.280899, Training Accuracy= 54.75556\n",
      "Fold: 1, Iter: 12, Loss= 4.125197, Training Accuracy= 65.37778\n",
      "Fold: 1, Iter: 14, Loss= 30.802694, Training Accuracy= 50.00000\n",
      "Fold: 1, Iter: 16, Loss= 40.380424, Training Accuracy= 50.00000\n",
      "Fold: 1, Iter: 18, Loss= 4.374770, Training Accuracy= 71.91111\n",
      "Fold: 1, Iter: 20, Loss= 19.928288, Training Accuracy= 52.23333\n",
      "Fold: 1, Iter: 22, Loss= 23.959681, Training Accuracy= 52.32222\n",
      "Fold: 1, Iter: 24, Loss= 15.549137, Training Accuracy= 60.52222\n",
      "Fold: 1, Iter: 26, Loss= 12.676518, Training Accuracy= 66.11111\n",
      "Fold: 1, Iter: 28, Loss= 5.490156, Training Accuracy= 77.65556\n",
      "Fold: 1, Iter: 30, Loss= 3.676546, Training Accuracy= 81.22222\n",
      "Fold: 1, Cross Validation Accuracy= 80.60000\n",
      "\n",
      "\n",
      "Confusion Matrix Training Set\n",
      "Predicted     0     1   All\n",
      "True                       \n",
      "0          3152   342  3494\n",
      "1          1348  4158  5506\n",
      "All        4500  4500  9000\n",
      "\n",
      "Confusion Matrix CrossValid Set\n",
      "Predicted    0    1   All\n",
      "True                     \n",
      "0          453  147   600\n",
      "1           47  353   400\n",
      "All        500  500  1000\n",
      "\n",
      "Validation Data and Labels shape:  (1000, 576) (1000, 2)\n",
      "Training Data and Labels shape:  (9000, 576) (9000, 2)\n",
      "The Label Dictionary is given as:  {0: 'trainDataAirplane.pickle', 1: 'trainDataCat.pickle'}\n",
      "Fold: 2, Iter: 2, Loss= 94.387215, Training Accuracy= 50.00000\n",
      "Fold: 2, Iter: 4, Loss= 106.665016, Training Accuracy= 50.00000\n",
      "Fold: 2, Iter: 6, Loss= 31.398472, Training Accuracy= 50.00000\n",
      "Fold: 2, Iter: 8, Loss= 54.616371, Training Accuracy= 50.00000\n",
      "Fold: 2, Iter: 10, Loss= 31.744032, Training Accuracy= 50.00000\n",
      "Fold: 2, Iter: 12, Loss= 34.804340, Training Accuracy= 50.00000\n",
      "Fold: 2, Iter: 14, Loss= 35.789181, Training Accuracy= 50.00000\n",
      "Fold: 2, Iter: 16, Loss= 33.234867, Training Accuracy= 50.00000\n",
      "Fold: 2, Iter: 18, Loss= 26.450045, Training Accuracy= 51.32222\n",
      "Fold: 2, Iter: 20, Loss= 21.606438, Training Accuracy= 55.87778\n",
      "Fold: 2, Iter: 22, Loss= 14.140860, Training Accuracy= 64.61111\n",
      "Fold: 2, Iter: 24, Loss= 7.759491, Training Accuracy= 73.90000\n",
      "Fold: 2, Iter: 26, Loss= 4.633465, Training Accuracy= 78.95556\n",
      "Fold: 2, Iter: 28, Loss= 3.422759, Training Accuracy= 80.96667\n",
      "Fold: 2, Iter: 30, Loss= 3.210464, Training Accuracy= 79.47778\n",
      "Fold: 2, Cross Validation Accuracy= 69.80000\n",
      "\n",
      "\n",
      "Confusion Matrix Training Set\n",
      "Predicted     0     1   All\n",
      "True                       \n",
      "0          2929   276  3205\n",
      "1          1571  4224  5795\n",
      "All        4500  4500  9000\n",
      "\n",
      "Confusion Matrix CrossValid Set\n",
      "Predicted    0    1   All\n",
      "True                     \n",
      "0          482  284   766\n",
      "1           18  216   234\n",
      "All        500  500  1000\n",
      "\n",
      "Validation Data and Labels shape:  (1000, 576) (1000, 2)\n",
      "Training Data and Labels shape:  (9000, 576) (9000, 2)\n",
      "The Label Dictionary is given as:  {0: 'trainDataAirplane.pickle', 1: 'trainDataCat.pickle'}\n",
      "Fold: 3, Iter: 2, Loss= 94.544647, Training Accuracy= 50.00000\n",
      "Fold: 3, Iter: 4, Loss= 107.386017, Training Accuracy= 50.00000\n",
      "Fold: 3, Iter: 6, Loss= 29.991577, Training Accuracy= 50.00000\n",
      "Fold: 3, Iter: 8, Loss= 52.683716, Training Accuracy= 50.00000\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    # Both the predictions and the labels should be in One-Hot vector format.\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/ predictions.shape[0])\n",
    "\n",
    "def confusionMatrix(predictions, labels):\n",
    "    # Both the predictions and the labels should be in One-Hot vector format.\n",
    "    return (pd.crosstab(np.argmax(labels, 1), np.argmax(predictions, 1), rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "\n",
    "class SessionExec():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.epochs = 30\n",
    "        \n",
    "    def model(self, trainDataIN, trainLabelsIN, validDataIN, validLabelsIN):\n",
    "#         print (self.trainGraphDict)\n",
    "#         print ('')\n",
    "#         print (self.validGraphDict)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "\n",
    "            for epoch in range(self.epochs): \n",
    "                feed_dict = {self.trainGraphDict['trainData']: trainDataIN,\n",
    "                             self.trainGraphDict['trainLabels']: trainLabelsIN\n",
    "                        }\n",
    "\n",
    "\n",
    "                _, loss, tpred = sess.run([self.trainGraphDict['optimizer'], \n",
    "                                                        self.trainGraphDict['lossCE'],\n",
    "                                                        self.trainGraphDict['trainPred']], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "                if (epoch+1)%2 == 0:\n",
    "                    # Evaluate training set\n",
    "                    tacc = accuracy(tpred, trainLabelsIN)\n",
    "                    print (\"Fold: \" + str(self.foldNUM+1) + \", Iter: \" + str(epoch+1) + \", Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(tacc))\n",
    "\n",
    "                    \n",
    "            # After all epoch are over with, test the model with the cross validation set\n",
    "            feed_dict = {self.validGraphDict['validData']: validDataIN,\n",
    "                     self.validGraphDict['validLabels']: validLabelsIN\n",
    "                }\n",
    "\n",
    "            vpred = sess.run(self.validGraphDict['validPred'], feed_dict=feed_dict)\n",
    "            \n",
    "            # Evaluate corss-validation set\n",
    "            vacc = accuracy(vpred, validLabelsIN)\n",
    "            print (\"Fold: \" + str(self.foldNUM+1) + \", Cross Validation Accuracy= \" + \"{:.5f}\".format(vacc))\n",
    "\n",
    "            print ('')\n",
    "            print ('')\n",
    "            \n",
    "            return tpred, tacc, vpred, vacc \n",
    "\n",
    "\n",
    "\n",
    "    def execute(self):\n",
    "        meanValidAcc = 0\n",
    "        for foldNUM, (trainDataIN, trainLabelsIN, validDataIN, validLabelsIN, labelDict) in enumerate(genTrainValidFolds(featureDIR, oneHot=True)):\n",
    "            print ('')\n",
    "            print ('Validation Data and Labels shape: ', validDataIN.shape, validLabelsIN.shape)\n",
    "            print ('Training Data and Labels shape: ', trainDataIN.shape, trainLabelsIN.shape)\n",
    "            print ('The Label Dictionary is given as: ', labelDict)\n",
    "            self.foldNUM = foldNUM\n",
    "            \n",
    "            reset_graph()\n",
    "            objNNET = BuildNeuralNet()\n",
    "            self.trainGraphDict = objNNET.trainNet()\n",
    "            self.validGraphDict = objNNET.crossValid()\n",
    "            tpred, tacc, vpred, vacc = self.model(trainDataIN, trainLabelsIN, validDataIN, validLabelsIN)\n",
    "            \n",
    "            trainCM = confusionMatrix(trainLabelsIN,tpred)\n",
    "            validCM = confusionMatrix(validLabelsIN,vpred)\n",
    "\n",
    "            meanValidAcc += vacc\n",
    "            print ('Confusion Matrix Training Set')\n",
    "            print (trainCM)\n",
    "            print ('')\n",
    "            print ('Confusion Matrix CrossValid Set')\n",
    "            print (validCM) \n",
    "#             break\n",
    "        print ('')\n",
    "        print ('The Mean crossValidation Accuracy is: ', meanValidAcc/(foldNUM+1))\n",
    "            \n",
    "            \n",
    "SessionExec().execute()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Running i is : 0\n",
    "# Validation Data and Labels shape:  (1000, 162) (1000, 2)\n",
    "# Training Data and Labels shape:  (9000, 162) (9000, 2)\n",
    "# The Label Dictionary is given as:  {0: 'trainDataAirplane.pickle', 1: 'trainDataCat.pickle'}\n",
    "# Fold: 1, Iter: 10, Loss= 1.603019, Training Accuracy= 0.79000\n",
    "# Fold: 1, Cross Validation Accuracy= 0.79200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def accuracy(predictions, labels):\n",
    "#     return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "#           / predictions.shape[0])\n",
    "\n",
    "\n",
    "# epochs = 30\n",
    "# def model(trainGraphDict, validGraphDict):\n",
    "#     print (trainGraphDict)\n",
    "#     print ('')\n",
    "#     print (validGraphDict)\n",
    "    \n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "#         for foldNUM, (trainDataIN, trainLabelsIN, validDataIN, validLabelsIN, labelDict) in enumerate(genTrainValidFolds(featureDIR, oneHot=True)):\n",
    "#             print ('Validation Data and Labels shape: ', validDataIN.shape, validLabelsIN.shape)\n",
    "#             print ('Training Data and Labels shape: ', trainDataIN.shape, trainLabelsIN.shape)\n",
    "#             print ('The Label Dictionary is given as: ', labelDict)\n",
    "\n",
    "#             for epoch in range(epochs): \n",
    "#                 feed_dict = {trainGraphDict['trainData']: trainDataIN,\n",
    "#                              trainGraphDict['trainLabels']: trainLabelsIN\n",
    "#                         }\n",
    "                \n",
    "                \n",
    "#                 _, loss, tacc, twghts, tbiases = sess.run([trainGraphDict['optimizer'], \n",
    "#                                                         trainGraphDict['lossCE'], \n",
    "#                                                         trainGraphDict['accuracy'],\n",
    "#                                                         trainGraphDict['weightsLRND'],\n",
    "#                                                         trainGraphDict['biasesLRND']], feed_dict=feed_dict)\n",
    "\n",
    "#                 if (epoch+1)%10 == 0:\n",
    "#                     print (\"Fold: \" + str(foldNUM+1) + \", Iter: \" + str(epoch) + \", Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(tacc))\n",
    "\n",
    "#                 # During the Last iteration of the nth fold batch, we should calculate the model accuracy with Cross Validataion Data\n",
    "#                 if epoch==epochs-1:\n",
    "#                     print ('')\n",
    "\n",
    "#                     feed_dict = {validGraphDict['validData']: validDataIN,\n",
    "#                              validGraphDict['validLabels']: validLabelsIN\n",
    "#                         }\n",
    "                \n",
    "#                     vacc = sess.run(validGraphDict['validAccuracy'], feed_dict=feed_dict)\n",
    "#                     print (\"Fold: \" + str(foldNUM+1) + \", Cross Validation Accuracy= \" + \"{:.5f}\".format(vacc))\n",
    "                    \n",
    "#             print ('')\n",
    "#             print ('')\n",
    "\n",
    "# #             break\n",
    "# #             epoch += 1\n",
    "# #         print (\"Optimization Finished!\")\n",
    "# #     obj_SVM = Models()\n",
    "# #     batchEvalDict[foldNUM] = obj_SVM.classify(trainData, trainLabels, validData)\n",
    "\n",
    "# reset_graph()\n",
    "# objNNET = BuildNeuralNet()\n",
    "# trainGraphDict = objNNET.trainNet()\n",
    "# validGraphDict = objNNET.crossValid()\n",
    "# model(trainGraphDict, validGraphDict)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
