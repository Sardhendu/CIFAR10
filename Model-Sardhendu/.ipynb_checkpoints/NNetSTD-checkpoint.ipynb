{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import Packages:\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import os, sys\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "mydir = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "sys.path.append(mydir)\n",
    "from DataGenerator import genTrainValidFolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STDbatch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Classification-1/STD/batchData/\"\n",
    "EDGbatch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Classification-1/EDG/batchData/\"\n",
    "HOGp1batch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Classification-1/HOGp1/batchData/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def toOneHotVector():\n",
    "    \n",
    "\n",
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "def reshape_data(dataset, labels, featureSize, numLabels, sample_size=None):\n",
    "    if sample_size:\n",
    "        dataset = dataset[:sample_size].reshape(sample_size, featureSize) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(numLabels) == labels[:,None]).astype(np.float32)\n",
    "    else:\n",
    "        dataset = dataset.reshape(len(dataset), featureSize) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(numLabels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BuildNeuralNet():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        seed = 2316\n",
    "        self.featureSize = 162\n",
    "        self.numHidden1 = 1000\n",
    "        self.numHidden2 = 1000\n",
    "        self.numHidden3 = 1000\n",
    "        self.numLabels = 2\n",
    "        self.alpha = 0.03\n",
    "        self.momentum = 0.9\n",
    "        \n",
    "        self.weights = {\n",
    "            \"inp_to_hid1_wght\": tf.Variable(tf.random_normal([self.featureSize, self.numHidden1], seed=seed)),\n",
    "            \"hid1_to_hid2_wght\" : tf.Variable(tf.random_normal([self.numHidden1, self.numHidden2], seed=seed)),\n",
    "            \"hid2_to_hid3_wght\": tf.Variable(tf.random_normal([self.numHidden2, self.numHidden3], seed=seed)),\n",
    "            \"hid3_to_out_wght\": tf.Variable(tf.random_normal([self.numHidden3, self.numLabels], seed=seed))\n",
    "        }\n",
    "        \n",
    "        self.biases = {\n",
    "            \"hid1_bias\": tf.Variable(tf.random_normal([self.numHidden1], seed=seed)),\n",
    "            \"hid2_bias\": tf.Variable(tf.random_normal([self.numHidden2], seed=seed)),\n",
    "            \"hid3_bias\": tf.Variable(tf.random_normal([self.numHidden3], seed=seed)),\n",
    "            \"out_bias\": tf.Variable(tf.random_normal([self.numLabels], seed=seed))\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def nNet(self):\n",
    "        trainData = tf.placeholder(tf.float32, [None, self.featureSize])\n",
    "        trainLabels = tf.placeholder(tf.float32, [None, self.numLabels])\n",
    "\n",
    "        # 1st Hidden Layer State\n",
    "        inp_to_hid1 = tf.matmul(trainData, self.weights['inp_to_hid1_wght']) + self.biases['hid1_bias']\n",
    "        hid1State = tf.sigmoid(inp_to_hid1)\n",
    "        \n",
    "        # 2nd Hidden Layer State\n",
    "        hid1_to_hid2 = tf.matmul(hid1State, self.weights['hid1_to_hid2_wght']) + self.biases['hid2_bias']\n",
    "        hid2State = tf.sigmoid(hid1_to_hid2)\n",
    "        \n",
    "        # 3nd Hidden Layer State\n",
    "        hid2_to_hid3 = tf.matmul(hid2State, self.weights['hid2_to_hid3_wght']) + self.biases['hid3_bias']\n",
    "        hid3State = tf.sigmoid(hid2_to_hid3)\n",
    "        \n",
    "        # Output Layer State\n",
    "        hid3_to_out = tf.matmul(hid3State, self.weights['hid3_to_out_wght']) + self.biases['out_bias']\n",
    "        outState = tf.nn.softmax(hid3_to_out)\n",
    "        \n",
    "        # Loss Function and Optimization\n",
    "        lossCE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hid3_to_out, labels=trainLabels))\n",
    "#         optimizer = tf.train.AdamOptimizer(learning_rate=self.alpha).minimize(lossCE)\n",
    "        optimizer = tf.train.MomentumOptimizer(self.alpha, \n",
    "                                            self.momentum, \n",
    "                                            use_locking=False, \n",
    "                                            name='Momentum', \n",
    "                                            use_nesterov=True).minimize(lossCE)\n",
    "        \n",
    "        # Evaluate model\n",
    "        correct_pred = tf.equal(tf.argmax(outState, 1), tf.argmax(trainLabels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        return dict(\n",
    "            trainData = trainData, \n",
    "            trainLabels = trainLabels,\n",
    "            optimizer = optimizer,\n",
    "            lossCE = lossCE,\n",
    "            accuracy = accuracy\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trainLabels': <tf.Tensor 'Placeholder_1:0' shape=(?, 2) dtype=float32>, 'accuracy': <tf.Tensor 'Mean_1:0' shape=() dtype=float32>, 'lossCE': <tf.Tensor 'Mean:0' shape=() dtype=float32>, 'trainData': <tf.Tensor 'Placeholder:0' shape=(?, 162) dtype=float32>, 'optimizer': <tensorflow.python.framework.ops.Operation object at 0x11d901cc0>}\n",
      "Running i is : 0\n",
      "Validation Data and Labels shape:  (1000, 162) (1000, 2)\n",
      "Training Data and Labels shape:  (9000, 162) (9000, 2)\n",
      "The Label Dictionary is given as:  {0: 'trainDataAirplane.pickle', 1: 'trainDataCat.pickle'}\n",
      "Iter 0, Minibatch Loss= 8.119655, Training Accuracy= 0.50700\n",
      "Iter 1, Minibatch Loss= 10.969023, Training Accuracy= 0.49444\n",
      "Iter 2, Minibatch Loss= 3.147324, Training Accuracy= 0.58311\n",
      "Iter 3, Minibatch Loss= 2.886378, Training Accuracy= 0.60944\n",
      "Iter 4, Minibatch Loss= 3.395161, Training Accuracy= 0.64133\n",
      "Iter 5, Minibatch Loss= 2.871589, Training Accuracy= 0.65478\n",
      "Iter 6, Minibatch Loss= 2.460382, Training Accuracy= 0.72211\n",
      "Iter 7, Minibatch Loss= 1.793284, Training Accuracy= 0.75767\n",
      "Iter 8, Minibatch Loss= 1.639954, Training Accuracy= 0.78022\n",
      "Iter 9, Minibatch Loss= 1.603019, Training Accuracy= 0.79000\n",
      "Iter 10, Minibatch Loss= 1.569964, Training Accuracy= 0.79589\n",
      "Iter 11, Minibatch Loss= 1.535357, Training Accuracy= 0.80100\n",
      "Iter 12, Minibatch Loss= 1.497784, Training Accuracy= 0.80400\n",
      "Iter 13, Minibatch Loss= 1.457155, Training Accuracy= 0.80811\n",
      "Iter 14, Minibatch Loss= 1.414268, Training Accuracy= 0.81111\n",
      "Iter 15, Minibatch Loss= 1.369966, Training Accuracy= 0.81633\n",
      "Iter 16, Minibatch Loss= 1.324851, Training Accuracy= 0.81778\n",
      "Iter 17, Minibatch Loss= 1.279587, Training Accuracy= 0.81833\n",
      "Iter 18, Minibatch Loss= 1.234840, Training Accuracy= 0.82167\n",
      "Iter 19, Minibatch Loss= 1.191227, Training Accuracy= 0.82289\n",
      "Iter 20, Minibatch Loss= 1.149524, Training Accuracy= 0.82522\n",
      "Iter 21, Minibatch Loss= 1.110455, Training Accuracy= 0.82444\n",
      "Iter 22, Minibatch Loss= 1.074516, Training Accuracy= 0.82456\n",
      "Iter 23, Minibatch Loss= 1.042036, Training Accuracy= 0.82456\n",
      "Iter 24, Minibatch Loss= 1.013256, Training Accuracy= 0.82522\n",
      "Iter 25, Minibatch Loss= 0.988228, Training Accuracy= 0.82456\n",
      "Iter 26, Minibatch Loss= 0.966572, Training Accuracy= 0.82144\n",
      "Iter 27, Minibatch Loss= 0.947561, Training Accuracy= 0.82244\n",
      "Iter 28, Minibatch Loss= 0.930419, Training Accuracy= 0.82189\n",
      "Iter 29, Minibatch Loss= 0.914501, Training Accuracy= 0.82100\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "def train(graphDict):\n",
    "    print (graphDict)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for foldNUM, (trainDataIN, trainLabelsIN, validDataIN, validLabelsIN, labelDict) in enumerate(genTrainValidFolds(HOGp1batch_dir, oneHot=True)):\n",
    "            print ('Validation Data and Labels shape: ', validDataIN.shape, validLabelsIN.shape)\n",
    "            print ('Training Data and Labels shape: ', trainDataIN.shape, trainLabelsIN.shape)\n",
    "            print ('The Label Dictionary is given as: ', labelDict)\n",
    "\n",
    "#             batchData, batchLabels = reshape_data(trainDataIN, trainLabelsIN, featureSize=162, numLabels=2)\n",
    "            for epoch in range(epochs): \n",
    "                feed_dict = {graphDict['trainData']: trainDataIN,\n",
    "                             graphDict['trainLabels']: trainLabelsIN\n",
    "                        }\n",
    "\n",
    "                _, loss, acc = sess.run([graphDict['optimizer'], graphDict['lossCE'], graphDict['accuracy']], feed_dict=feed_dict)\n",
    "                print (\"Iter \" + str(epoch) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "            break\n",
    "#             epoch += 1\n",
    "#         print (\"Optimization Finished!\")\n",
    "#     obj_SVM = Models()\n",
    "#     batchEvalDict[foldNUM] = obj_SVM.classify(trainData, trainLabels, validData)\n",
    "\n",
    "reset_graph()\n",
    "graphDict = BuildNeuralNet().nNet()\n",
    "train(graphDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
