{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Packages:\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "# Model Packeges import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "mydir = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "sys.path.append(mydir)\n",
    "from DataGenerator import genTrainValidFolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STDbatch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/STD/batchData/\"\n",
    "EDGbatch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/EDG/batchData/\"\n",
    "HOGp1batch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/HOGp1/batchData/\"   # dim =162\n",
    "HOGp2batch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/HOGp2/batchData/\"   # dim =576\n",
    "HOGp3batch_dir = \"/Users/sam/All-Program/App-DataSet/Kaggle-Challenges/CIFAR-10/Model-Sardhendu/HOGp3/batchData/\"\n",
    "\n",
    "featureDIR = HOGp1batch_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def toOneHotVector():\n",
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "def reshape_data(dataset, labels, featureSize, numLabels, sample_size=None):\n",
    "    if sample_size:\n",
    "        dataset = dataset[:sample_size].reshape(sample_size, featureSize) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(numLabels) == labels[:,None]).astype(np.float32)\n",
    "    else:\n",
    "        dataset = dataset.reshape(len(dataset), featureSize) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(numLabels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BuildNeuralNet():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.seed = 2316\n",
    "        self.featureSize = 72     # Note to change the featureSize when using a new Feature Set\n",
    "        self.numHidden1 = 512\n",
    "        self.numHidden2 = 1024\n",
    "        self.numHidden3 = 1024\n",
    "        self.numLabels = 2\n",
    "        self.alpha = 0.06          # Learning Rate \n",
    "        self.beta = 0.01          # Penalty for regularization\n",
    "        self.momentum = 0.9\n",
    "        \n",
    "    def __initWghts__(self):\n",
    "        self.weights = {\n",
    "            \"inp_to_hid1_wght\": tf.Variable(tf.random_normal([self.featureSize, self.numHidden1], seed=self.seed)),\n",
    "            \"hid1_to_hid2_wght\" : tf.Variable(tf.random_normal([self.numHidden1, self.numHidden2], seed=self.seed)),\n",
    "            \"hid2_to_hid3_wght\": tf.Variable(tf.random_normal([self.numHidden2, self.numHidden3], seed=self.seed)),\n",
    "            \"hid3_to_out_wght\": tf.Variable(tf.random_normal([self.numHidden3, self.numLabels], seed=self.seed))\n",
    "        }\n",
    "        \n",
    "        self.biases = {\n",
    "            \"hid1_bias\": tf.Variable(tf.random_normal([self.numHidden1], seed=self.seed)),\n",
    "            \"hid2_bias\": tf.Variable(tf.random_normal([self.numHidden2], seed=self.seed)),\n",
    "            \"hid3_bias\": tf.Variable(tf.random_normal([self.numHidden3], seed=self.seed)),\n",
    "            \"out_bias\": tf.Variable(tf.random_normal([self.numLabels], seed=self.seed))\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def trainNet(self, regularization=False):\n",
    "        self.__initWghts__()\n",
    "        \n",
    "        trainData = tf.placeholder(tf.float32, [None, self.featureSize])\n",
    "        trainLabels = tf.placeholder(tf.float32, [None, self.numLabels])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        # 1st Hidden Layer State\n",
    "        inp_to_hid1 = tf.matmul(trainData, self.weights['inp_to_hid1_wght']) + self.biases['hid1_bias']\n",
    "        hid1State = tf.sigmoid(inp_to_hid1)\n",
    "        hid1State = tf.nn.dropout(hid1State, keep_prob)\n",
    "        \n",
    "        # 2nd Hidden Layer State\n",
    "        hid1_to_hid2 = tf.matmul(hid1State, self.weights['hid1_to_hid2_wght']) + self.biases['hid2_bias']\n",
    "        hid2State = tf.sigmoid(hid1_to_hid2)\n",
    "        hid2State = tf.nn.dropout(hid2State, keep_prob)\n",
    "        \n",
    "        # 3nd Hidden Layer State\n",
    "        hid2_to_hid3 = tf.matmul(hid2State, self.weights['hid2_to_hid3_wght']) + self.biases['hid3_bias']\n",
    "        hid3State = tf.sigmoid(hid2_to_hid3)\n",
    "#         hid3State = tf.nn.dropout(hid3State, keep_prob)\n",
    "        \n",
    "        # Output Layer State\n",
    "        hid3_to_out = tf.matmul(hid3State, self.weights['hid3_to_out_wght']) + self.biases['out_bias']\n",
    "        outState = tf.nn.softmax(hid3_to_out)\n",
    "        \n",
    "        # Loss Function\n",
    "        lossCE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hid3_to_out, labels=trainLabels))\n",
    "        \n",
    "        # Loss function using L2 Regularization\n",
    "        if regularization:\n",
    "            regularizers = (tf.nn.l2_loss(self.weights['inp_to_hid1_wght']) \n",
    "                            + tf.nn.l2_loss(self.weights['hid1_to_hid2_wght']) \n",
    "                            + tf.nn.l2_loss(self.weights['hid2_to_hid3_wght']) \n",
    "                            + tf.nn.l2_loss(self.weights['hid3_to_out_wght'])\n",
    "                        )\n",
    "            lossCE = tf.reduce_mean(lossCE + (self.beta * regularizers))\n",
    "        \n",
    "        # Optimizer\n",
    "#         optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(lossCE)\n",
    "#         optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(lossCE)\n",
    "        optimizer = tf.train.MomentumOptimizer(self.alpha, \n",
    "                                            self.momentum, \n",
    "                                            use_locking=False, \n",
    "                                            name='Momentum', \n",
    "                                            use_nesterov=True).minimize(lossCE)\n",
    "#         optimizer = tf.train.AdagradOptimizer(learning_rate=0.04).minimize(lossCE)\n",
    "#         optimizer = tf.train.RMSPropOptimizer(learning_rate=0.01, momentum=0.9).minimize(lossCE)   # This setting seems to be good\n",
    "    \n",
    "    \n",
    "        \n",
    "        # Evaluate model\n",
    "#         trainPred = tf.equal(tf.argmax(outState, 1), tf.argmax(trainLabels, 1))\n",
    "#         trainAccuracy = tf.reduce_mean(tf.cast(trainPred, tf.float32))\n",
    "        \n",
    "        return dict(\n",
    "            trainData = trainData, \n",
    "            trainLabels = trainLabels,\n",
    "            keep_prob = keep_prob,\n",
    "            weightsLRND = self.weights,\n",
    "            biasesLRND = self.biases,\n",
    "            trainPred = outState,\n",
    "            optimizer = optimizer,\n",
    "            lossCE = lossCE\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def crossValid(self, wLRND, bLRND):\n",
    "\n",
    "        validData = tf.placeholder(tf.float32, [None, self.featureSize])\n",
    "        validLabels = tf.placeholder(tf.float32, [None, self.numLabels])\n",
    "\n",
    "        # 1st Hidden Layer State\n",
    "        valid_hid1State = tf.sigmoid(tf.matmul(validData, wLRND['inp_to_hid1_wght']) + bLRND['hid1_bias'])\n",
    "\n",
    "        # 2nd Hidden Layer State\n",
    "        valid_hid2State = tf.sigmoid(tf.matmul(valid_hid1State, wLRND['hid1_to_hid2_wght']) + bLRND['hid2_bias'])\n",
    "\n",
    "        # 3nd Hidden Layer State\n",
    "        valid_hid3State = tf.sigmoid(tf.matmul(valid_hid2State, wLRND['hid2_to_hid3_wght']) + bLRND['hid3_bias'])\n",
    "\n",
    "        # Output Layer State\n",
    "        validOutState = tf.nn.softmax(tf.matmul(valid_hid3State, wLRND['hid3_to_out_wght']) + bLRND['out_bias'])\n",
    "\n",
    "    #             validPred = tf.equal(tf.argmax(validOutState, 1), tf.argmax(validLabels, 1))\n",
    "    #             validAccuracy = tf.reduce_mean(tf.cast(validPred, tf.float32))\n",
    "\n",
    "        return dict(\n",
    "            validData = validData,\n",
    "            validLabels = validLabels,\n",
    "            validPred = validOutState\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Data and Labels shape:  (1000, 72) (1000, 2)\n",
      "Training Data and Labels shape:  (9000, 72) (9000, 2)\n",
      "The Label Dictionary is given as:  {0: 'trainDataAirplane.pickle', 1: 'trainDataCat.pickle'}\n",
      "Fold: 1, Iter: 2, Loss= 9.775234, Training Accuracy= 49.41111\n",
      "Fold: 1, Iter: 4, Loss= 7.674893, Training Accuracy= 50.23333\n",
      "Fold: 1, Iter: 6, Loss= 7.850288, Training Accuracy= 50.33333\n",
      "Fold: 1, Iter: 8, Loss= 7.803713, Training Accuracy= 50.78889\n",
      "Fold: 1, Iter: 10, Loss= 8.223995, Training Accuracy= 51.16667\n",
      "Fold: 1, Iter: 12, Loss= 7.649767, Training Accuracy= 51.88889\n",
      "Fold: 1, Iter: 14, Loss= 7.453847, Training Accuracy= 52.40000\n",
      "Fold: 1, Iter: 16, Loss= 7.378599, Training Accuracy= 52.23333\n",
      "Fold: 1, Iter: 18, Loss= 6.969594, Training Accuracy= 52.75556\n",
      "Fold: 1, Iter: 20, Loss= 7.059536, Training Accuracy= 53.42222\n",
      "Fold: 1, Iter: 22, Loss= 7.012105, Training Accuracy= 53.82222\n",
      "Fold: 1, Iter: 24, Loss= 6.836982, Training Accuracy= 53.58889\n",
      "Fold: 1, Iter: 26, Loss= 6.662841, Training Accuracy= 54.91111\n",
      "Fold: 1, Iter: 28, Loss= 6.832151, Training Accuracy= 54.40000\n",
      "ENtering the last iteration..... 29\n",
      "Fold: 1, Iter: 30, Loss= 6.902870, Training Accuracy= 54.34444\n",
      "Fold: 1, Cross Validation Accuracy= 53.80000\n",
      "\n",
      "\n",
      "Confusion Matrix Training Set\n",
      "Predicted     0     1   All\n",
      "True                       \n",
      "0          3864  3473  7337\n",
      "1           636  1027  1663\n",
      "All        4500  4500  9000\n",
      "\n",
      "Confusion Matrix CrossValid Set\n",
      "Predicted    0    1   All\n",
      "True                     \n",
      "0           39    1    40\n",
      "1          461  499   960\n",
      "All        500  500  1000\n",
      "\n",
      "Validation Data and Labels shape:  (1000, 72) (1000, 2)\n",
      "Training Data and Labels shape:  (9000, 72) (9000, 2)\n",
      "The Label Dictionary is given as:  {0: 'trainDataAirplane.pickle', 1: 'trainDataCat.pickle'}\n",
      "Fold: 2, Iter: 2, Loss= 9.490187, Training Accuracy= 50.32222\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    # Both the predictions and the labels should be in One-Hot vector format.\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/ predictions.shape[0])\n",
    "\n",
    "def confusionMatrix(predictions, labels):\n",
    "    # Both the predictions and the labels should be in One-Hot vector format.\n",
    "    return (pd.crosstab(np.argmax(labels, 1), np.argmax(predictions, 1), rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "\n",
    "class SessionExec():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.epochs = 30\n",
    "        \n",
    "    def trainModel(self, trainDataIN, trainLabelsIN):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "\n",
    "            for epoch in range(self.epochs): \n",
    "                feed_dict = {self.trainGraphDict['trainData']: trainDataIN,\n",
    "                             self.trainGraphDict['trainLabels']: trainLabelsIN,\n",
    "                             self.trainGraphDict['keep_prob']: 0.7\n",
    "                        }\n",
    "\n",
    "                if epoch == self.epochs-1:  # Capture the weights for the last iteration.\n",
    "                    print ('ENtering the last iteration.....', epoch)\n",
    "                    wLRND, bLRND, _, loss, tpred = sess.run([self.trainGraphDict['weightsLRND'],\n",
    "                                                self.trainGraphDict['biasesLRND'],\n",
    "                                                self.trainGraphDict['optimizer'],\n",
    "                                                self.trainGraphDict['lossCE'],\n",
    "                                                self.trainGraphDict['trainPred']], feed_dict=feed_dict)\n",
    "                else:\n",
    "                    _, loss, tpred = sess.run([self.trainGraphDict['optimizer'], \n",
    "                                                self.trainGraphDict['lossCE'],\n",
    "                                                self.trainGraphDict['trainPred']], feed_dict=feed_dict)\n",
    "\n",
    "                if (epoch+1)%2 == 0 or epoch==self.epochs-1:\n",
    "                    # Evaluate training set\n",
    "                    tacc = accuracy(tpred, trainLabelsIN)\n",
    "                    print (\"Fold: \" + str(self.foldNUM+1) + \", Iter: \" + str(epoch+1) + \", Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(tacc))\n",
    "    \n",
    "#             print (wLRND)\n",
    "#             print ('')\n",
    "#             print (bLRND)\n",
    "            return tpred, tacc, wLRND, bLRND\n",
    "\n",
    "                \n",
    "    # After all epoch are over with, test the model with the cross validation set    \n",
    "    def validateModel(self, validDataIN, validLabelsIN):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            \n",
    "            feed_dict = {self.validGraphDict['validData']: validDataIN,\n",
    "                     self.validGraphDict['validLabels']: validLabelsIN\n",
    "                }\n",
    "\n",
    "            vpred = sess.run(self.validGraphDict['validPred'], feed_dict=feed_dict)\n",
    "            \n",
    "            # Evaluate corss-validation set\n",
    "            vacc = accuracy(vpred, validLabelsIN)\n",
    "            print (\"Fold: \" + str(self.foldNUM+1) + \", Cross Validation Accuracy= \" + \"{:.5f}\".format(vacc))\n",
    "\n",
    "            print ('')\n",
    "            print ('')\n",
    "            \n",
    "            return vpred, vacc \n",
    "        \n",
    "    \n",
    "    \n",
    "    def sumWghtBias(self, cmnDictIN, newDictIN):\n",
    "        for param, val_aray in newDictIN.items():\n",
    "            cmnDictIN[param] = np.add(val_aray, cmnDictIN[param])    \n",
    "        return cmnDictIN\n",
    "            \n",
    "        \n",
    "    def execute(self, updWghtBias=False):\n",
    "        numFolds = 10\n",
    "        meanValidAcc = 0\n",
    "        self.wLRND = {}\n",
    "        self.bLRND = {}\n",
    "        for foldNUM, (trainDataIN, trainLabelsIN, validDataIN, validLabelsIN, labelDict) in enumerate(genTrainValidFolds(featureDIR, oneHot=True)):\n",
    "            print ('')\n",
    "            print ('Validation Data and Labels shape: ', validDataIN.shape, validLabelsIN.shape)\n",
    "            print ('Training Data and Labels shape: ', trainDataIN.shape, trainLabelsIN.shape)\n",
    "            print ('The Label Dictionary is given as: ', labelDict)\n",
    "            self.foldNUM = foldNUM\n",
    "            \n",
    "            reset_graph()\n",
    "            \n",
    "            # Build a class Object\n",
    "            objNNET = BuildNeuralNet()\n",
    "            \n",
    "            # Build the training Graph\n",
    "            self.trainGraphDict = objNNET.trainNet(regularization=False)\n",
    "            tpred, tacc, wLRND, bLRND = self.trainModel(trainDataIN, trainLabelsIN)\n",
    "            \n",
    "            \n",
    "            reset_graph()\n",
    "            \n",
    "            # Add all the weights and biases received from each training fold\n",
    "            if updWghtBias:\n",
    "                if not (self.wLRND and self.bLRND):\n",
    "                    self.wLRND = wLRND\n",
    "                    self.bLRND = bLRND\n",
    "                else:\n",
    "                    self.wLRND = self.sumWghtBias(self.wLRND, wLRND)\n",
    "                    self.bLRND = self.sumWghtBias(self.bLRND, bLRND)\n",
    "\n",
    "                    \n",
    "            self.validGraphDict = objNNET.crossValid(wLRND, bLRND)\n",
    "            vpred, vacc = self.validateModel(validDataIN, validLabelsIN)\n",
    "            \n",
    "            trainCM = confusionMatrix(trainLabelsIN,tpred)\n",
    "            validCM = confusionMatrix(validLabelsIN,vpred)\n",
    "\n",
    "            meanValidAcc += vacc\n",
    "            print ('Confusion Matrix Training Set')\n",
    "            print (trainCM)\n",
    "            print ('')\n",
    "            print ('Confusion Matrix CrossValid Set')\n",
    "            print (validCM)\n",
    "            \n",
    "#             if foldNUM ==2:\n",
    "#                 break\n",
    "        \n",
    "        # Test the cross validation accuracy for the nth-fold when the weights are averaged \n",
    "        # Find the average of all the weights and biases from the training folds and take their average\n",
    "        if updWghtBias:\n",
    "            wLRND = {k: v/(foldNUM+1) for k, v in self.wLRND.items()} \n",
    "            bLRND = {k: v/(foldNUM+1) for k, v in self.bLRND.items()} \n",
    "            self.validGraphDict = objNNET.crossValid(wLRND, bLRND)\n",
    "            vpred, vacc = self.validateModel(validDataIN, validLabelsIN)\n",
    "            print (\"Fold: \" + str(self.foldNUM+1) + \", Aveaged Weight Cross Validation Accuracy= \" + \"{:.5f}\".format(vacc))\n",
    "        \n",
    "        print ('')\n",
    "        print ('The Mean crossValidation Accuracy is: ', meanValidAcc/(foldNUM+1))\n",
    "            \n",
    "            \n",
    "SessionExec().execute(updWghtBias=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Running i is : 0\n",
    "# Validation Data and Labels shape:  (1000, 162) (1000, 2)\n",
    "# Training Data and Labels shape:  (9000, 162) (9000, 2)\n",
    "# The Label Dictionary is given as:  {0: 'trainDataAirplane.pickle', 1: 'trainDataCat.pickle'}\n",
    "# Fold: 1, Iter: 10, Loss= 1.603019, Training Accuracy= 0.79000\n",
    "# Fold: 1, Cross Validation Accuracy= 0.79200"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
